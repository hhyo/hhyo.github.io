<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL字段类型TEXT和BLOB的应用]]></title>
    <url>%2F2018%2F11%2F29%2Fmysql-text-blob%2F</url>
    <content type="text"><![CDATA[InnoDB行格式Compact Compac行格式如果blob列值长度 &lt;= 768 bytes，就不会发生行溢出(page overflow)，内容都在数据页(B-tree Node)；如果列值长度 &gt; 768字节，那么前768字节依然在数据页，而剩余的则放在溢出页(off-page)，如下图： 上面所讲的讲的blob或变长大字段类型包括blob,text,varchar，其中varchar列值长度大于某数N时也会存溢出页，在latin1字符集下N值可以这样计算：innodb的块大小默认为16kb，由于innodb存储引擎表为索引组织表，树底层的叶子节点为一双向链表，因此每个页中至少应该有两行记录，这就决定了innodb在存储一行数据的时候不能够超过8k，减去其它列值所占字节数，约等于N。 我们知道对于InnoDB来说，内存是极为珍贵的，如果把768字节长度的blob都放在数据页，虽然可以节省部分IO，但相对来说能缓存行数就变少，也就是能缓存的索引值变少了，降低了索引效率。 Dynamic和Compressed Dynamic行格式对blob采用完全行溢出，即聚集索引记录（数据页）只保留20字节的指针，指向真实存放它的溢出段地址 dynamic行格式，列存储是否放到off-page页，主要取决于行大小，它会把行中最长的那一列放到off-page，直到数据页能存放下两行。TEXT/BLOB列 &lt;=40 bytes 时总是存放于数据页。这种方式可以避免compact那样把太多的大列值放到 B-tree Node，因为dynamic格式认为，只要大列值有部分数据放在off-page，那把整个值放入都放入off-page更有效 compressed 物理结构上与dynamic类似，但是对表的数据行使用zlib算法进行了压缩存储。在long blob列类型比较多的情况下用，可以降低off-page的使用，减少存储空间（一般40%左右），但要求更高的CPU，buffer pool里面可能会同时存储数据的压缩版和非压缩版，所以也多占用部分内存。 字符串数据类型 BINARY 和 VARBINARY BINARY和VARBINARY与 CHAR和VARCHAR类型有点类似，不同的是BINARY和VARBINARY存储的是二进制的字符串，而非字符型字符串。也就是说，BINARY和VARBINARY没有字符集的概念，对其排序和比较都是按照二进制值进行对比。 BINARY（N）和VARBINARY（N）中的N指的是字节长度，而CHAR（N）和VARCHAR（N）中N指的是的字符长度。对于BINARY（10） ，其可存储的字节固定为10，而对于CHAR（10） ，其可存储的字节视字符集的情况而定。 BLOB and TEXT 可以将BLOB 列视为VARBINARY、将 TEXT列视为VARCHAR 对于索引BLOB和 TEXT列，必须指定索引前缀长度 BLOB和TEXT列不能有DEFAULT值 BLOB 可以储存图片，TEXT只能储存纯文本文件 排序只对每个列的最前max_sort_length字节而不是整个字符串排序 MEMORY存储引擎不支持BLOB和TEXT，如果查询使用了BLOB或TEXT列并且需要使用临时表，则会使用磁盘临时表，导致性能下降（5.7.5开始，新增一个系统选项 internal_tmp_disk_storage_engine 可定义磁盘临时表的引擎类型为InnoDB） 存取和优化拆分将大字段从频繁读取和更新的表中拆分出去 压缩将数据压缩后再存为BLOB， 使用CPU换取内存和IO 覆盖索引常用查询字段建立索引，直接在索引上面完成查询，不需要将大字段读读取 参考&emsp;&emsp;1. InnoDB行格式对text/blob大变长字段的影响]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL使用SSL连接]]></title>
    <url>%2F2018%2F11%2F22%2Fmysql-ssl%2F</url>
    <content type="text"><![CDATA[测试版本1234567mysql&gt; select version();+------------+| version() |+------------+| 5.7.22-log |+------------+1 row in set (0.01 sec) 5.7默认会开启SSL，需要关闭可以在配置文件增加skip_ssl 检测当前实例是否开启SSL123456789101112131415mysql&gt; show global variables like &apos;%ssl%&apos;; +---------------+----------+| Variable_name | Value |+---------------+----------+| have_openssl | DISABLED || have_ssl | DISABLED || ssl_ca | || ssl_capath | || ssl_cert | || ssl_cipher | || ssl_crl | || ssl_crlpath | || ssl_key | |+---------------+----------+9 rows in set (0.01 sec) SSL是关闭的 使用mysql_ssl_rsa_setup创建证书与私钥MySQL5.7提供了mysql_ssl_rsa_setup工具，可以很方便的创建配置SSL所需要的各个文件 123456789101112131415161718192021222324mysql_ssl_rsa_setup --helpmysql_ssl_rsa_setup Version : 1.0.0 Distribution : 5.7.22 For : osx10.13 On : x86_64Copyright (c) 2015, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.MySQL SSL Certificate and RSA Key Generation UtilityUsage : mysql_ssl_rsa_setup [OPTIONS] -?, --help Display this help and exit. -v, --verbose Be more verbose when running program -V, --version Print program version and exit -d, --datadir=name Directory to store generated files. -s, --suffix=name Suffix to be added in certificate subject line --uid=name The effective user id to be used for file permissionVariables (--variable-name=value)and boolean options &#123;FALSE|TRUE&#125; Value (after reading options)--------------------------------- ----------------------------------------verbose FALSEdatadir /usr/local/var/mysqlsuffix 5.7.20uid (No default value) 创建相关文件-d可以选择文件保存到哪个文件夹，这里我将文件保存在当前目录12345678910111213141516➜ ~ mysql_ssl_rsa_setup -d ./ Generating a 2048 bit RSA private key......+++...........................................+++writing new private key to &apos;ca-key.pem&apos;-----Generating a 2048 bit RSA private key................................................................+++................................................................................................................................................................................................................+++writing new private key to &apos;server-key.pem&apos;-----Generating a 2048 bit RSA private key...+++..............................+++writing new private key to &apos;client-key.pem&apos;----- 查看生成的文件 123456789➜ ~ ll *.pem ca-key.pem #CA 证书, 用于生成服务器端/客户端的数字证书ca.pem #CA 私钥, 用于生成服务器端/客户端的数字证书client-cert.pem #服务器端的 RSA 私钥client-key.pem #服务器端的证书请求文件, 用于生成服务器端的数字证书private_key.pem #服务器端的数字证书public_key.pem #客户端的 RSA 私钥server-cert.pem #客户端的证书请求文件, 用于生成客户端的数字证书server-key.pem #客户端的数字证书 SSL配置服务端配置配置证书位置服务端需要ca.pem、server-cert.pem 、server-key.pem 三个文件，在配置文件添加 1234[mysqld]ssl-ca=/var/lib/mysql/ca.pemssl-cert=/var/lib/mysql/server-cert.pemssl-key=/var/lib/mysql/server-key.pem 重启服务，查看SSL已经开启 123456789101112131415mysql&gt; show global variables like &apos;%ssl%&apos;;+---------------+--------------------------------+| Variable_name | Value |+---------------+--------------------------------+| have_openssl | YES || have_ssl | YES || ssl_ca | /var/lib/mysql/ca.pem || ssl_capath | || ssl_cert | /var/lib/mysql/server-cert.pem || ssl_cipher | || ssl_crl | || ssl_crlpath | || ssl_key | /var/lib/mysql/server-key.pem |+---------------+--------------------------------+9 rows in set (0.00 sec) 创建REQUIRE SSL账号12mysql&gt; grant all on *.* to ssl_test identified by &apos;123456&apos; require ssl;Query OK, 0 rows affected, 1 warning (0.02 sec) 客户端配置拷贝ca.pem、client-cert.pem、client-key.pem文件到客户端机器 指定证书位置连接123456789101112131415161718192021222324252627282930313233343536373839➜ data mysql --ssl-ca=./ca.pem --ssl-cert=./client-cert.pem --ssl-key=./client-key.pem -h 127.0.0.1 -u ssl_test -P33061 -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.22-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; \s--------------mysql Ver 14.14 Distrib 5.7.22, for osx10.13 (x86_64) using EditLine wrapperConnection id: 7Current database:Current user: ssl_test@172.21.0.1SSL: Cipher in use is DHE-RSA-AES256-SHACurrent pager: lessUsing outfile: &apos;&apos;Using delimiter: ;Server version: 5.7.22-log MySQL Community Server (GPL)Protocol version: 10Connection: 127.0.0.1 via TCP/IPServer characterset: utf8Db characterset: utf8Client characterset: utf8Conn. characterset: utf8TCP port: 33061Uptime: 7 min 47 secThreads: 3 Questions: 19 Slow queries: 0 Opens: 111 Flush tables: 1 Open tables: 104 Queries per second avg: 0.040--------------mysql&gt; SSL: Cipher in use is DHE-RSA-AES256-SHA表示已经使用 SSL 来连接了 DISABLED SSL连接会提示密码错误 123➜ data mysql --ssl-mode=DISABLED -h 127.0.0.1 -u ssl_test -P33061 -pEnter password: ERROR 1045 (28000): Access denied for user &apos;ssl_test&apos;@&apos;172.21.0.1&apos; (using password: YES) 使用配置文件连接在配置文件添加如下信息1234[client]ssl-ca=/path/ca.pemssl-cert=/path/client-cert.pemssl-key=/path/client-key.pem 则在命令行连接时会默认使用配置文件的证书信息 Replication使用SSL配置将Master对应的客户端证书文件ca.pem、client-cert.pem、client-key.pem拷贝到SLAVE机器，然后在CHANGE MASTER时指定证书位置，并且开启MASTER_SSL配置 12345678910mysql&gt; CHANGE MASTER TO -&gt; MASTER_HOST=&apos;m1&apos;, -&gt; MASTER_USER=&apos;ssl_test&apos;, -&gt; MASTER_PASSWORD=&apos;123456&apos;, -&gt; MASTER_AUTO_POSITION=1, -&gt; MASTER_SSl=1, -&gt; MASTER_SSl_CA=&apos;/var/lib/mysql/ca.pem&apos;, -&gt; MASTER_SSl_CERT=&apos;/var/lib/mysql/client-cert.pem&apos;, -&gt; MASTER_SSl_KEY=&apos;/var/lib/mysql/client-key.pem&apos;;Query OK, 0 rows affected, 2 warnings (0.04 sec) 查看SLAVE信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061mysql&gt; show slave status\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: m1 Master_User: ssl_test Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000008 Read_Master_Log_Pos: 482 Relay_Log_File: relay-bin.000002 Relay_Log_Pos: 414 Relay_Master_Log_File: mysql-bin.000008 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 482 Relay_Log_Space: 671 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: Yes Master_SSL_CA_File: /var/lib/mysql/ca.pem Master_SSL_CA_Path: Master_SSL_Cert: /var/lib/mysql/client-cert.pem Master_SSL_Cipher: Master_SSL_Key: /var/lib/mysql/client-key.pem Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: 82ba33f9-ee1f-11e8-b4e5-0242ac150002 Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: 82b862a0-ee1f-11e8-b241-0242ac150003:1-13,82ba33f9-ee1f-11e8-b4e5-0242ac150002:1-83 Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) 错误解决Last_IO_Errno: 2026123Master_SSL_Allowed: YesLast_IO_Errno: 2026Last_IO_Error: error connecting to master &apos;ssl_test@m1:3306&apos; - retry-time: 60 retries: 1 SSL证书验证不通过，确认Master和Slave的证书信息是否匹配 Last_IO_Errno: 1045有时候在确认账号密码正确的情况下，查看slave信息依然提示Last_IO_Errno: 1045，这实际上是因为在CHANGE_MASTER时Master_SSL没有开启123Master_SSL_Allowed: NoLast_IO_Errno: 1045Last_IO_Error: error connecting to master &apos;ssl_test@m1:3306&apos; - retry-time: 60 retries: 1 开启Master_SSL即可 12345678mysql&gt; stop slave;Query OK, 0 rows affected (0.01 sec)mysql&gt; change master to master_ssl=1;Query OK, 0 rows affected (0.03 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec) ERROR 2026 (HY000): SSL connection error: error:14082174:SSL routines:ssl3_check_cert_and_algorithm:dh key too smallopenssl 1.0.1e-30.el6_6.9系bug：https://bugzilla.redhat.com/show_bug.cgi?id=1228755https://access.redhat.com/errata/RHBA-2015:1129 mysql也做了修复：Fixed also in MySQL development version 5.7 in 5.7.6:https://github.com/mysql/mysql-server/commit/866b988a76e8e7e217017a7883a52a12ec5024b9http://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-6.html This fix updates DH params to 2048bits rather than 1024bits. 参考&emsp;&emsp;1. MySQL 使用 SSL 连接(附 Docker 例子)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SSL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL复制过滤测试]]></title>
    <url>%2F2018%2F11%2F22%2Fmysql-replication-filter%2F</url>
    <content type="text"><![CDATA[版本和配置MySQL: 5.7.22-logbinlog-format=ROWslave_exec_mode=IDEMPOTENTMaster-Slave基于GTID的主从复制Master-Slave实例包含库test_c、test_b、test_c 复制过滤参数123456--replicate-do-db--replicate-ignore-db--replicate-do-table--replicate-wild-do-table--replicate-ignore-table--replicate-wild-ignore-table 参数说明可参考：http://dp.imysql.com:8080/node/58 Database-Level Replication 流程图 Table-Level Replication 流程图 测试库级过滤Slave配置过滤规则：replicate-do-db=test_a DDLCREATE/DROP TABLEMaster执行 123456789101112mysql&gt; use test_b;Database changedmysql&gt; create table test_a.t (pk int primary key);Query OK, 0 rows affected (0.03 sec)mysql&gt; use test_a;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; drop table t;Query OK, 0 rows affected (0.01 sec) Slave复制报错 12Last_SQL_Errno: 1051Last_SQL_Error: Error &apos;Unknown table &apos;test_a.t&apos;&apos; on query. Default database: &apos;test_a&apos;. Query: &apos;DROP TABLE `t` /* generated by server */&apos; 查看master binlog信息 12345678910mysql&gt; show binlog events in &apos;mysql-bin.000004&apos; from 1340;+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------+| mysql-bin.000004 | 1340 | Gtid | 1 | 1405 | SET @@SESSION.GTID_NEXT= &apos;82ba33f9-ee1f-11e8-b4e5-0242ac150002:40&apos; || mysql-bin.000004 | 1405 | Query | 1 | 1525 | use `test_b`; create table test_a.t (pk int primary key) || mysql-bin.000004 | 1525 | Gtid | 1 | 1590 | SET @@SESSION.GTID_NEXT= &apos;82ba33f9-ee1f-11e8-b4e5-0242ac150002:41&apos; || mysql-bin.000004 | 1590 | Query | 1 | 1708 | use `test_a`; DROP TABLE `t` /* generated by server */ |+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------+4 rows in set (0.01 sec) ALTER TABLEMaster执行 12345mysql&gt; use test_b;Database changed mysql&gt; alter table test_a.t add v varchar(10) not null default &apos;&apos;;Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0 Slave复制正常查看slave binlog信息 1234567mysql&gt; show binlog events in &apos;mysql-bin.000012&apos; from 2301;+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------------+| mysql-bin.000012 | 2301 | Query | 1 | 2437 | use `test_b`; alter table test_a.t add v varchar(10) not null default &apos;&apos; |+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------------+1 row in set (0.01 sec) DMLMaster执行 1234567mysql&gt; use test_b;Database changedmysql&gt; mysql&gt; insert into test_a.t values(4);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t values(8);Query OK, 1 row affected (0.00 sec) Slave复制正常 查看master binlog信息 12345678910111213141516mysql&gt; show binlog events in &apos;mysql-bin.000004&apos; from 1893;+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------+| mysql-bin.000004 | 1893 | Gtid | 1 | 1958 | SET @@SESSION.GTID_NEXT= &apos;82ba33f9-ee1f-11e8-b4e5-0242ac150002:43&apos; || mysql-bin.000004 | 1958 | Query | 1 | 2032 | BEGIN || mysql-bin.000004 | 2032 | Table_map | 1 | 2078 | table_id: 114 (test_a.t) || mysql-bin.000004 | 2078 | Write_rows | 1 | 2118 | table_id: 114 flags: STMT_END_F || mysql-bin.000004 | 2118 | Xid | 1 | 2149 | COMMIT /* xid=87 */ || mysql-bin.000004 | 2149 | Gtid | 1 | 2214 | SET @@SESSION.GTID_NEXT= &apos;82ba33f9-ee1f-11e8-b4e5-0242ac150002:44&apos; || mysql-bin.000004 | 2214 | Query | 1 | 2288 | BEGIN || mysql-bin.000004 | 2288 | Table_map | 1 | 2334 | table_id: 108 (test_b.t) || mysql-bin.000004 | 2334 | Write_rows | 1 | 2374 | table_id: 108 flags: STMT_END_F || mysql-bin.000004 | 2374 | Xid | 1 | 2405 | COMMIT /* xid=89 */ |+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------+10 rows in set (0.00 sec) 表级过滤Slave配置过滤规则：replicate-do-table=test_a.t DDLCREATE/DROP TABLEMaster执行 123456789101112mysql&gt; use test_b;Database changedmysql&gt; create table test_a.t (pk int primary key);Query OK, 0 rows affected (0.03 sec)mysql&gt; use test_a;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; drop table t;Query OK, 0 rows affected (0.01 sec) Slave复制正常 查看Slave的binlog信息 12345678910mysql&gt; mysql&gt; show binlog events in &apos;mysql-bin.000012&apos; from 1683;+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------+| mysql-bin.000012 | 1683 | Gtid | 1 | 1748 | SET @@SESSION.GTID_NEXT= &apos;82ba33f9-ee1f-11e8-b4e5-0242ac150002:52&apos; || mysql-bin.000012 | 1748 | Query | 1 | 1868 | use `test_b`; create table test_a.t (pk int primary key) || mysql-bin.000012 | 1868 | Gtid | 1 | 1933 | SET @@SESSION.GTID_NEXT= &apos;82ba33f9-ee1f-11e8-b4e5-0242ac150002:53&apos; || mysql-bin.000012 | 1933 | Query | 1 | 2051 | use `test_a`; DROP TABLE `t` /* generated by server */ |+------------------+------+------------+-----------+-------------+--------------------------------------------------------------------+4 rows in set (0.01 sec) ALTER TABLEMaster执行 12345678910111213mysql&gt; use test_bDatabase changedmysql&gt; alter table test_a.t add v varchar(10) not null default &apos;&apos;;Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; use test_a;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; alter table test_a.t drop v;Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0 Slave复制正常 查看Slave的binlog信息 12345678910111213mysql&gt; show binlog events in &apos;mysql-bin.000012&apos;;+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------------------+| mysql-bin.000012 | 4 | Format_desc | 2 | 123 | Server ver: 5.7.22-log, Binlog ver: 4 || mysql-bin.000012 | 123 | Previous_gtids | 2 | 234 | 82b862a0-ee1f-11e8-b241-0242ac150003:1-8,82ba33f9-ee1f-11e8-b4e5-0242ac150002:1-44 || mysql-bin.000012 | 234 | Gtid | 1 | 299 | SET @@SESSION.GTID_NEXT= &apos;82ba33f9-ee1f-11e8-b4e5-0242ac150002:45&apos; || mysql-bin.000012 | 299 | Query | 1 | 435 | use `test_b`; alter table test_a.t add v varchar(10) not null default &apos;&apos; || mysql-bin.000012 | 435 | Gtid | 1 | 500 | SET @@SESSION.GTID_NEXT= &apos;82ba33f9-ee1f-11e8-b4e5-0242ac150002:46&apos; || mysql-bin.000012 | 500 | Query | 1 | 605 | use `test_a`; alter table test_a.t drop v |+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------------------+6 rows in set (0.01 sec) DMLMaster执行 1234567891011mysql&gt; use test_bReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; insert into test_a.t values(10); Query OK, 1 row affected (0.01 sec)mysql&gt; update t t1,test_a.t t2 set t2.v=t1.pk; Query OK, 2 rows affected (0.00 sec)Rows matched: 2 Changed: 2 Warnings: 0 Slave复制正常 库表级规则混用组合多，仅列举一种：do-db &amp; do-tableSlave配置过滤规则：replicate-do-db=test_areplicate-do-table=test_a.t DDLCREATE/DROP TABLEMaster执行 123456789101112131415mysql&gt; use test_bReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; create table test_a.t (pk int primary key);Query OK, 0 rows affected (0.02 sec)mysql&gt; use test_a;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; drop table t;Query OK, 0 rows affected (0.01 sec) Slave复制报错 12Last_SQL_Errno: 1051Last_SQL_Error: Error &apos;Unknown table &apos;test_a.t&apos;&apos; on query. Default database: &apos;test_a&apos;. Query: &apos;DROP TABLE `t` /* generated by server */&apos; ALTER TABLEMaster执行 1234567891011121314151617mysql&gt; use test_bReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; alter table test_a.t add v varchar(10) not null default &apos;&apos;;Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; use test_a;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; alter table test_a.t drop v;Query OK, 0 rows affected (0.03 sec)Records: 0 Duplicates: 0 Warnings: 0 Slave复制报错 12Last_SQL_Errno: 1091 Last_SQL_Error: Error &apos;Can&apos;t DROP &apos;v&apos;; check that column/key exists&apos; on query. Default database: &apos;test_a&apos;. Query: &apos;alter table test_a.t drop v&apos; DMLMaster执行 12345678910mysql&gt; use test_bReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; insert into test_a.t values(10);Query OK, 1 row affected (0.01 sec)mysql&gt; delete from test_a.t where pk=10;Query OK, 1 row affected (0.00 sec) Slave复制正常 结论 当BINLOG以ROW格式记录执行语句时，复制过滤允许跨库执行语句，不以当前默认数据库（用USE语句指定）为主，当BINLOG以STATEMENT格式记录执行语句时，复制过滤以当前默认数据库（用USE语句指定）为主（不允许跨库），可参考上面两次DDL测试，注意：注只有DML语句可以使用ROW格式记录，DDL语句始终以STATEMENT形式记录 库表级别过滤混合使用时，当库级别检测不符合时会直接返回，不会再走到表级别判断，因此建议使用–replicate-wild-do-table=test_a.%来替换–replicate-do-db=test_a做库级过滤，可防止跨库的DDL(CREATE)操作导致Slave复制报错 参考 &emsp;&emsp;1：Mysql 复制过滤详解&emsp;&emsp;2：MySQL主从复制过滤规则应用]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>replicate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL复制参数slave_exec_mode=IDEMPOTENT测试]]></title>
    <url>%2F2018%2F11%2F22%2Fslave-exec-mode%2F</url>
    <content type="text"><![CDATA[参数定义可以在主从复制中遇到 duplicate-key 和 no-key-found 错误时，自动覆盖或者略过binlog里面这个row_event，避免报错停止复制，可以动态修改 测试结论看这里吧，总结的很好了：http://seanlook.com/2018/03/11/mysql-replication-error-and-idempotent/]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>replicate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7下RPM安装MHA0.58]]></title>
    <url>%2F2018%2F06%2F20%2Fmha-install%2F</url>
    <content type="text"><![CDATA[准备RPM包12https://github.com/yoshinorim/mha4mysql-manager/releaseshttps://github.com/yoshinorim/mha4mysql-node/releases 安装依赖12yum install epel-release -yyum -y install perl-DBD-MySQL perl-Config-Tiny perl-Log-Dispatch perl-Parallel-ForkManager perl-Time-HiRes 安装node1rpm -ivh mha4mysql-node-0.58-0.el7.centos.noarch.rpm #安装manager 1rpm -ivh mha4mysql-manager-0.58-0.el7.centos.noarch.rpm]]></content>
      <categories>
        <category>mha</category>
      </categories>
      <tags>
        <tag>mha</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ProxySQL+Keepalived高可用方案]]></title>
    <url>%2F2018%2F05%2F25%2Fproxysql-ha%2F</url>
    <content type="text"><![CDATA[基础信息ProxySQL版本：1.4.8-32-g669c149ProxySQL 1：proxysql1 192.168.1.15 监听端口6032ProxySQL 2：proxysql2 192.168.1.16 监听端口6032 Keepalived版本：keepalived-1.4.4同时在ProxySQL 1和ProxySQL 2上面安装 VIP192.168.1.88 机器信息1234# cat /etc/redhat-releaseCentOS Linux release 7.4.1708 (Core) # uname -aLinux 192-168-1-16 3.10.0-693.5.2.el7.x86_64 #1 SMP Fri Oct 20 20:32:50 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux 安装Keepalived12345678# 依赖安装yum install curl gcc openssl-devel libnl3-devel net-snmp-devel# 下载wget &quot;https://rpmfind.net/linux/centos/7.5.1804/os/x86_64/Packages/keepalived-1.3.5-6.el7.x86_64.rpm&quot;# 安装Keepalivedrpm -ivh keepalived-1.3.5-6.el7.x86_64.rpm 配置两个keepalived配置 12ProxySQL 1: state MASTER;priority 100ProxySQL 2: state BACKUP;priority 90 123456789101112131415161718192021222324252627282930313233343536373839404142434445vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id PROXY_HA vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;# Script used to check if Proxy is runningvrrp_script check_proxy &#123; #检测proxysql服务是否在运行。有很多方式，比如进程，用脚本检测等等 script &quot;killall -0 proxysql&quot; #这里通过命令监测 interval 2 #检查执行间隔，每2s检测一次 weight -30 #结果导致的优先级变更，检测失败（返回非0）则优先级 -30 fall 2 #检测连续2次失败才算确定是真失败。会用weight减少优先级（1-255之间） rise 1 #检测1次成功就算成功。但不修改优先级&#125;# Virtual interface# The priority specifies the order in which the assigned interface to take over in a failovervrrp_instance VI_1 &#123; state MASTER #实例状态，只有MASTER 和 BACKUP两种状态，并且需要全部大写。抢占模式下，其中MASTER为工作状态，BACKUP为备用状态。当MASTER所在的服务器失效时，BACKUP所在的服务会自动把它的状态由BACKUP切换到MASTER状态。当失效的MASTER所在的服务恢复时，BACKUP从MASTER恢复到BACKUP状态。 interface eth0 #指定虚拟ip的网卡接口 virtual_router_id 91 #路由器标识，MASTER和BACKUP必须是一致的，可选择IP最后一段使用，相同的 VRID 为一个组，他将决定多播的 MAC 地址。 priority 100 #定义优先级，数字越大，优先级越高，在同一个vrrp_instance下，MASTER的优先级必须大于BACKUP的优先级。这样MASTER故障恢复后，就可以将VIP资源再次抢回来 #nopreempt #禁止抢占服务。默认情况，当MASTER服务挂掉之后，BACKUP自动升级为MASTER并接替它的任务，当MASTER服务恢复后，升级为MASTER的BACKUP服务又自动降为BACKUP，把工作权交给原MASTER。当配置了nopreempt，MASTER从挂掉到恢复，不再将服务抢占过来。注意不要开，会影响多次主备切换 advert_int 1 #MASTER与BACKUP节点间同步检查的时间间隔，单位为秒 virtual_ipaddress &#123; 172.16.200.88 #VIP &#125; track_script &#123; check_proxy #状态检测Script &#125;&#125; 配置iptables防火墙规则123456vim /etc/sysconfig/iptables-A INPUT -i eth0 -d 224.0.0.18 -j ACCEPT #允许组播地址通信 -A INPUT -i eth0 -p vrrp -j ACCEPT #允许VRRP（虚拟路由器冗余协）通信 -A INPUT -p tcp -m state --state NEW -m tcp --dport 6033 -j ACCEPT #开放proxysql的6033端口 -A INPUT -p tcp -m state --state NEW -m tcp --dport 6032 -j ACCEPT #开放proxysql的6032管理端口 重启防火墙 1systemctl restart iptables 测试启动keepalived1systemctl start keepalived 查看vip切换情况1234567891011ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether fa:3e:6e:ff:b7:00 brd ff:ff:ff:ff:ff:ff inet 192.168.1.15/16 brd 192.168.255.255 scope global eth0 valid_lft forever preferred_lft forever inet 191.168.1.88/32 scope global eth0 valid_lft forever preferred_lft forever 通过VIP连接12345678910111213141516mysql -u proxysql -p -h vip -P6033Enter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 26Server version: 5.5.30 (ProxySQL)Copyright (c) 2009-2017 Percona LLC and/or its affiliatesCopyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.proxysql:(none)&gt; 停止proxysql1的proxysql进程12# 停止proxysqlsystemctl stop proxysql 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 查看两台机器keepalived的日志# proxysql1systemctl status keepalived● keepalived.service - LVS and VRRP High Availability Monitor Loaded: loaded (/usr/lib/systemd/system/keepalived.service; disabled; vendor preset: disabled) Active: active (running) since 一 2018-05-28 15:56:07 CST; 49s ago Process: 23027 ExecStart=/usr/sbin/keepalived $KEEPALIVED_OPTIONS (code=exited, status=0/SUCCESS) Main PID: 23028 (keepalived) CGroup: /system.slice/keepalived.service ├─23028 /usr/sbin/keepalived -D ├─23029 /usr/sbin/keepalived -D └─23030 /usr/sbin/keepalived -D5月 28 15:56:49 172-16-200-15 Keepalived_vrrp[23030]: /usr/bin/killall -0 proxysql exited with status 15月 28 15:56:51 172-16-200-15 Keepalived_vrrp[23030]: /usr/bin/killall -0 proxysql exited with status 15月 28 15:56:51 172-16-200-15 Keepalived_vrrp[23030]: VRRP_Script(check_proxy) failed5月 28 15:56:52 172-16-200-15 Keepalived_vrrp[23030]: VRRP_Instance(VI_1) Changing effective priority from 100 to 705月 28 15:56:52 172-16-200-15 Keepalived_vrrp[23030]: VRRP_Instance(VI_1) Received advert with higher priority 90, ours 705月 28 15:56:52 172-16-200-15 Keepalived_vrrp[23030]: VRRP_Instance(VI_1) Entering BACKUP STATE5月 28 15:56:52 172-16-200-15 Keepalived_vrrp[23030]: VRRP_Instance(VI_1) removing protocol VIPs.5月 28 15:56:52 172-16-200-15 Keepalived_vrrp[23030]: VRRP_Instance(VI_1) removing protocol iptable drop rule5月 28 15:56:53 172-16-200-15 Keepalived_vrrp[23030]: /usr/bin/killall -0 proxysql exited with status 15月 28 15:56:55 172-16-200-15 Keepalived_vrrp[23030]: /usr/bin/killall -0 proxysql exited with status 1# proxysql2systemctl status keepalived● keepalived.service - LVS and VRRP High Availability Monitor Loaded: loaded (/usr/lib/systemd/system/keepalived.service; disabled; vendor preset: disabled) Active: active (running) since 一 2018-05-28 16:00:29 CST; 29s ago Process: 28006 ExecStart=/usr/sbin/keepalived $KEEPALIVED_OPTIONS (code=exited, status=0/SUCCESS) Main PID: 28007 (keepalived) CGroup: /system.slice/keepalived.service ├─28007 /usr/sbin/keepalived -D ├─28008 /usr/sbin/keepalived -D └─28009 /usr/sbin/keepalived -D5月 28 16:00:29 172-16-200-16 Keepalived_vrrp[28009]: WARNING - script `killall` resolved by path search to `/usr/bin/killall`. Please specify full path.5月 28 16:00:29 172-16-200-16 Keepalived_vrrp[28009]: SECURITY VIOLATION - scripts are being executed but script_security not enabled.5月 28 16:00:29 172-16-200-16 Keepalived_vrrp[28009]: VRRP_Instance(VI_1) removing protocol VIPs.5月 28 16:00:29 172-16-200-16 Keepalived_vrrp[28009]: VRRP_Instance(VI_1) removing protocol iptable drop rule5月 28 16:00:29 172-16-200-16 Keepalived_vrrp[28009]: Using LinkWatch kernel netlink reflector...5月 28 16:00:30 172-16-200-16 Keepalived_vrrp[28009]: VRRP_Instance(VI_1) Entering BACKUP STATE5月 28 16:00:30 172-16-200-16 Keepalived_vrrp[28009]: VRRP sockpool: [ifindex(2), proto(112), unicast(0), fd(10,11)]5月 28 16:00:30 172-16-200-16 Keepalived_vrrp[28009]: VRRP_Script(check_proxy) succeeded5月 28 16:00:57 172-16-200-16 Keepalived_vrrp[28009]: VRRP_Instance(VI_1) forcing a new MASTER election5月 28 16:00:58 172-16-200-16 Keepalived_vrrp[28009]: VRRP_Instance(VI_1) Transition to MASTER STATE 123456789101112131415161718192021222324# 查看两台机器的vip绑定情况# proxysql1ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether fa:3e:6e:ff:b7:00 brd ff:ff:ff:ff:ff:ff inet 172.16.200.15/16 brd 172.16.255.255 scope global eth0 valid_lft forever preferred_lft forever # proxysql2ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether fa:e2:15:64:5c:00 brd ff:ff:ff:ff:ff:ff inet 172.16.200.16/16 brd 172.16.255.255 scope global eth0 valid_lft forever preferred_lft forever inet 172.16.200.88/32 scope global eth0 valid_lft forever preferred_lft forever 可见已经实现vip的漂移，通过命令行连接 12345678910111213141516 mysql -u proxysql -p -h vip -P6033mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 3Server version: 5.5.30 (ProxySQL)Copyright (c) 2009-2017 Percona LLC and/or its affiliatesCopyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.proxysql:(none)&gt; 问题排查keepalived主备同时都有VIP原因：主备vvrp直接无法通信解决：检查防火墙，开启组播 主备切换后，新主再故障，主备无法再次切换原因：主开启了nopreempt，不抢占解决：去除主的该配置即可，可参考Keepalived中Master和Backup角色选举策略 参考&emsp;&emsp;1. Mysql+Keepalived双主热备高可用操作记录&emsp;&emsp;2. 解决ProxySQL的单点问题&emsp;&emsp;3. Keepalived中Master和Backup角色选举策略]]></content>
      <categories>
        <category>ProxySQL</category>
      </categories>
      <tags>
        <tag>ProxySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7下使用RPM安装ProxySQL以及Cluster搭建]]></title>
    <url>%2F2018%2F05%2F23%2Fproxysql-install%2F</url>
    <content type="text"><![CDATA[基础信息ProxySQL版本：1.4.8-32-g669c149ProxySQL 1：proxysql1 192.168.1.15 监听端口6032ProxySQL 2：proxysql2 192.168.1.16 监听端口6032 机器信息1234# cat /etc/redhat-releaseCentOS Linux release 7.4.1708 (Core) # uname -aLinux 192-168-1-16 3.10.0-693.5.2.el7.x86_64 #1 SMP Fri Oct 20 20:32:50 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux 安装安装依赖1yum install perl-DBD-MySQL 准备RPM包1wget &quot;https://github.com/sysown/proxysql/releases/download/v1.4.8/proxysql-1.4.8-1-centos7.x86_64.rpm&quot; 安装1rpm -ivh *.rpm 启动配置修改12345678910111213141516171819202122232425262728293031323334vim /etc/proxysql.cnfdatadir=&quot;/var/lib/proxysql&quot;admin_variables =&#123; admin_credentials=&quot;admin:admin;cluster1:secret1pass&quot; mysql_ifaces=&quot;0.0.0.0:6032&quot; cluster_username=&quot;cluster1&quot; cluster_password=&quot;secret1pass&quot; cluster_check_interval_ms=200 cluster_check_status_frequency=100 cluster_mysql_query_rules_save_to_disk=true cluster_mysql_servers_save_to_disk=true cluster_mysql_users_save_to_disk=true cluster_proxysql_servers_save_to_disk=true cluster_mysql_query_rules_diffs_before_sync=3 cluster_mysql_servers_diffs_before_sync=3 cluster_mysql_users_diffs_before_sync=3 cluster_proxysql_servers_diffs_before_sync=3&#125;proxysql_servers =( &#123; hostname=&quot;proxysql1&quot; port=6032 comment=&quot;proxysql1&quot; &#125;, &#123; hostname=&quot;proxysql2&quot; port=6032 comment=&quot;proxysql2&quot; &#125;) 1234# 启动systemctl start proxysql# 停止systemctl stop proxysql 连接123456789101112131415# 连接管理端口mysql -u admin -padmin -h 127.0.0.1 -P6032 --prompt=&apos;Admin&gt;&apos;mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.5.30 (ProxySQL Admin Module)Copyright (c) 2009-2017 Percona LLC and/or its affiliatesCopyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. 检查集群状态123456789101112131415161718192021Admin&gt;select * from proxysql_servers;+----------+------+--------+-----------+| hostname | port | weight | comment |+----------+------+--------+-----------+| master | 6032 | 0 | proxysql1 || slave | 6032 | 0 | proxysql2 |+----------+------+--------+-----------+2 rows in set (0.00 sec)Admin&gt;SELECT * FROM runtime_checksums_values ORDER BY name;+-------------------+---------+------------+--------------------+| name | version | epoch | checksum |+-------------------+---------+------------+--------------------+| admin_variables | 0 | 0 | || mysql_query_rules | 1 | 1527154033 | 0x0000000000000000 || mysql_servers | 1 | 1527154033 | 0x0000000000000000 || mysql_users | 1 | 1527154033 | 0x0000000000000000 || mysql_variables | 0 | 0 | || proxysql_servers | 1 | 1527154033 | 0x2F2F2BFD35FC2D9C |+-------------------+---------+------------+--------------------+6 rows in set (0.00 sec) version：代表LOAD TO RUNTIME执行了多少次，ProxySQL每次LOAD TO RUNTIME执行时都会生成一个新的配置校验epoch：LOAD TO RUNTIME执行时间的时间戳 #配置 1234567891011121314151617+-------------------------+| RUNTIME |+-------------------------+ /|\ | | | [1] | [2] | | \|/+-------------------------+| MEMORY |+-------------------------+ _ /|\ | |\ | | \ [3] | [4] | \ [5] | \|/ \+-------------------------+ +-------------------------+| DISK | | CONFIG FILE |+-------------------------+ +-------------------------+ 添加backends1234567891011Admin&gt;INSERT INTO mysql_servers(hostgroup_id,hostname,port) VALUES (1,&apos;master&apos;,3306),(2,&apos;slave&apos;,3306); Query OK, 1 row affected (0.00 sec)Admin&gt;SELECT * FROM mysql_servers;+--------------+----------+------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+| hostgroup_id | hostname | port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |+--------------+----------+------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+| 1 | master | 3306 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | || 2 | slave | 3306 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | |+--------------+----------+------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+2 rows in set (0.00 sec) 配置监控创建监控账号 12345678910111213141516171819202122232425262728293031323334353637# mysql实例添加账号GRANT SUPER, REPLICATION CLIENT ON *.* TO proxysql_monitor IDENTIFIED BY '12345678';# 修改proxysql配置Admin&gt;UPDATE global_variables SET variable_value='proxysql_monitor' WHERE variable_name='mysql-monitor_username';Query OK, 1 row affected (0.00 sec)Admin&gt;UPDATE global_variables SET variable_value='12345678' WHERE variable_name='mysql-monitor_password';Query OK, 1 row affected (0.00 sec)Admin&gt;SELECT * FROM global_variables WHERE variable_name LIKE 'mysql-monitor_%';+-----------------------------------------------------+------------------+| variable_name | variable_value |+-----------------------------------------------------+------------------+| mysql-monitor_enabled | true || mysql-monitor_connect_timeout | 600 || mysql-monitor_ping_max_failures | 3 || mysql-monitor_ping_timeout | 1000 || mysql-monitor_read_only_max_timeout_count | 3 || mysql-monitor_replication_lag_interval | 10000 || mysql-monitor_replication_lag_timeout | 1000 || mysql-monitor_groupreplication_healthcheck_interval | 5000 || mysql-monitor_groupreplication_healthcheck_timeout | 800 || mysql-monitor_replication_lag_use_percona_heartbeat | || mysql-monitor_query_interval | 60000 || mysql-monitor_query_timeout | 100 || mysql-monitor_slave_lag_when_null | 60 || mysql-monitor_wait_timeout | true || mysql-monitor_writer_is_also_reader | true || mysql-monitor_username | proxysql_monitor || mysql-monitor_password | 12345678 || mysql-monitor_history | 600000 || mysql-monitor_connect_interval | 60000 || mysql-monitor_ping_interval | 10000 || mysql-monitor_read_only_interval | 1500 || mysql-monitor_read_only_timeout | 500 |+-----------------------------------------------------+------------------+22 rows in set (0.01 sec) 应用和保存配置 1234567891011Admin&gt;LOAD MYSQL VARIABLES TO RUNTIME;Query OK, 0 rows affected (0.00 sec)Admin&gt;SAVE MYSQL VARIABLES TO DISK;Query OK, 94 rows affected (0.02 sec)Admin&gt;LOAD MYSQL SERVERS TO RUNTIME; Query OK, 0 rows affected (0.00 sec)Admin&gt;SAVE MYSQL SERVERS TO DISK;Query OK, 0 rows affected (0.08 sec) 配置用户12345678910111213141516171819202122232425262728293031323334353637383940414243444546Admin&gt;select * from global_variables where variable_name=&apos;admin-hash_passwords&apos;; +----------------------+----------------+| variable_name | variable_value |+----------------------+----------------+| admin-hash_passwords | true |+----------------------+----------------+1 row in set (0.01 sec)Admin&gt;SELECT * FROM mysql_users;Empty set (0.00 sec)Admin&gt;INSERT INTO mysql_users(username,password,default_hostgroup) VALUES (&apos;proxysql&apos;,&apos;12345678&apos;,1);Query OK, 1 row affected (0.00 sec)Admin&gt;SELECT username,password FROM mysql_users;+----------+----------+| username | password |+----------+----------+| proxysql | 12345678 |+----------+----------+1 row in set (0.00 sec)Admin&gt;LOAD MYSQL USERS TO RUNTIME;Query OK, 0 rows affected (0.00 sec)Admin&gt;SELECT username,password FROM mysql_users;+----------+----------+| username | password |+----------+----------+| proxysql | 12345678 |+----------+----------+1 row in set (0.00 sec)Admin&gt;SAVE MYSQL USERS FROM RUNTIME;Query OK, 0 rows affected (0.00 sec)Admin&gt;SELECT username,password FROM mysql_users;+----------+-------------------------------------------+| username | password |+----------+-------------------------------------------+| proxysql | *84AAC12F54AB666ECFC2A83C676908C8BBC381B1 |+----------+-------------------------------------------+1 row in set (0.00 sec)Admin&gt;SAVE MYSQL USERS TO DISK;Query OK, 0 rows affected (0.03 sec) 查看集群同步状态分别到proxysql1和proxysql2操作 123456789101112Admin&gt;SELECT * FROM runtime_checksums_values ORDER BY name;+-------------------+---------+------------+--------------------+| name | version | epoch | checksum |+-------------------+---------+------------+--------------------+| admin_variables | 0 | 0 | || mysql_query_rules | 1 | 1527154033 | 0x0000000000000000 || mysql_servers | 2 | 1527154464 | 0xCDFB3987855F1469 || mysql_users | 2 | 1527154515 | 0x77717322EF8E08EB || mysql_variables | 0 | 0 | || proxysql_servers | 1 | 1527154033 | 0x2F2F2BFD35FC2D9C |+-------------------+---------+------------+--------------------+6 rows in set (0.00 sec) 检查proxysql2的mysql_servers\mysql_users 1234567891011121314151617181920212223Admin&gt;select * from mysql_servers; +--------------+----------+------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+| hostgroup_id | hostname | port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |+--------------+----------+------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+| 1 | master | 3306 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | || 2 | slave | 3306 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | |+--------------+----------+------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+2 rows in set (0.00 sec)Admin&gt;select * from mysql_users;+----------+-------------------------------------------+--------+---------+-------------------+----------------+---------------+------------------------+--------------+---------+----------+-----------------+| username | password | active | use_ssl | default_hostgroup | default_schema | schema_locked | transaction_persistent | fast_forward | backend | frontend | max_connections |+----------+-------------------------------------------+--------+---------+-------------------+----------------+---------------+------------------------+--------------+---------+----------+-----------------+| proxysql | *84AAC12F54AB666ECFC2A83C676908C8BBC381B1 | 1 | 0 | 1 | | 0 | 1 | 0 | 0 | 1 | 10000 || proxysql | *84AAC12F54AB666ECFC2A83C676908C8BBC381B1 | 1 | 0 | 1 | | 0 | 1 | 0 | 1 | 0 | 10000 |+----------+-------------------------------------------+--------+---------+-------------------+----------------+---------------+------------------------+--------------+---------+----------+-----------------+2 rows in set (0.00 sec) 可以看到mysql_servers\mysql_users的版本号已经加1，但是admin_variables依然为0，因为目前proxysql仅支持如下四个模块的同步，其他模块数据变更需要每个实例单独修改 1234mysql_query_rulesmysql_serversmysql_usersproxysql_servers 运行日志查看1tail -f /var/lib/proxysql/proxysql.log 测试连接操作mysql1234567891011121314151617181920212223242526# 连接mysql -u proxysql -p -h 127.0.0.1 -P6033Enter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 18Server version: 5.5.30 (ProxySQL)Copyright (c) 2009-2017 Percona LLC and/or its affiliatesCopyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.proxysql:(none)&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.01 sec) 参考&emsp;&emsp;1. http://www.proxysql.com/blog/proxysql-cluster]]></content>
      <categories>
        <category>ProxySQL</category>
      </categories>
      <tags>
        <tag>ProxySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PGPool-II+PG流复制实现HA主备切换]]></title>
    <url>%2F2018%2F05%2F17%2Fpgpool-II-install%2F</url>
    <content type="text"><![CDATA[架构图 基础信息PostgreSQL版本：10.4主库db：192.168.1.15 监听端口5432备库db：192.168.1.16 监听端口5432vip：192.168.1.88 PGPool-II版本：3.7.3 机器信息1234# cat /etc/redhat-releaseCentOS Linux release 7.4.1708 (Core) # uname -aLinux 192-168-1-16 3.10.0-693.5.2.el7.x86_64 #1 SMP Fri Oct 20 20:32:50 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux 安装PGPool-II(RPM方式)安装依赖1yum -y install libmemcached-devel 准备RPM包下载地址 1234pgpool-II-pg10-3.7.3-1pgdg.rhel7.x86_64.rpmpgpool-II-pg10-debuginfo-3.7.3-1pgdg.rhel7.x86_64.rpmpgpool-II-pg10-devel-3.7.3-1pgdg.rhel7.x86_64.rpmpgpool-II-pg10-extensions-3.7.3-1pgdg.rhel7.x86_64.rpm 安装1rpm -ivh *.rpm 配置默认配置文件目录：/etc/pgpool-II 配置ssh秘钥配置秘钥使master和slave的postgres用户能免密连接 先修改postgres的密码，在root用户下 1passwd postgres 在master,slave机器上都生成ssh 1234[root@localhost ~]# su - postgres[postgres@localhost ~]$ ssh-keygen -t rsa[postgres@localhost ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys[postgres@localhost ~]$ chmod 600 ~/.ssh/authorized_keys 分别将master的公钥复制到slave，slave的公钥复制到master 配置系统命令权限配置 ifconfig, arping 执行权限 ，执行failover_stream.sh需要用到，可以让其他普通用户执行 12chmod u+s /sbin/ifconfig chmod u+s /usr/sbin 配置pcp.confpcp.conf是配置pgpool-II自己的用户名和密码，pgpool提供pcp接口，可以查看，管理pgpool的状态，并且可以远程操作pgpool 1234567# 使用pg_md5生成密码pg_md5 password5f4dcc3b5aa765d61d8327deb882cf99# 添加到pcp.conf配置文件vim pcp.conf# 编辑内容如下postgres:5f4dcc3b5aa765d61d8327deb882cf99 配置pool_hba.conf和PostgreSQL的pg_hba.conf文件一样， Pgpool-II使用名为pool_hba.conf的配置文件支持类似的客户端认证功能，要和pg的pg_hba.conf保持一致，要么都是trust，要么都是md5验证方式 1234567# TYPE DATABASE USER CIDR-ADDRESS METHOD# &quot;local&quot; is for Unix domain socket connections onlylocal all all md5# IPv4 local connections:host all all 0.0.0.0/0 md5host all all ::0/0 md5 在pgpool中添加pg数据库的用户名和密码由于Pgpool-II是一个在PostgreSQL服务器和PostgreSQL数据库客户端之间工作的中间件 ，所以当客户端应用程序连接到Pgpool-II时，Pgpool-II inturn会使用相同的凭据连接到PostgreSQL服务器，以便为客户端连接提供服务。执行如下命令，会在配置目录生成pool_passwd文件 123pg_md5 -p -m -u postgres pool_passwdcat pool_passwdpostgres:md5a3556571e93b0d20722ba62be61e8c2d 配置pgpool.confpgpool.conf是Pgpool-II的主要配置文件 创建健康检查用户12create user pgpool_healthchk with password &apos;xxxx&apos;;grant all privileges on database postgres to pgpool_healthchk; master配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124# CONNECTIONSlisten_addresses = &apos;*&apos;port = 9999pcp_listen_addresses = &apos;*&apos;pcp_port = 9898# - Backend Connection Settings -backend_hostname0 = &apos;master&apos;backend_port0 = 5432backend_weight0 = 1backend_data_directory0 = &apos;/opt/PostgreSQL/data&apos;backend_flag0 = &apos;ALLOW_TO_FAILOVER&apos;backend_hostname1 = &apos;slave&apos;backend_port1 = 5432backend_weight1 = 1backend_data_directory1 = &apos;/opt/PostgreSQL/data&apos;backend_flag1 = &apos;ALLOW_TO_FAILOVER&apos;# - Authentication -enable_pool_hba = onpool_passwd = &apos;pool_passwd&apos;# FILE LOCATIONSpid_file_name = &apos;/opt/pgpool/pgpool.pid&apos;replication_mode = offmaster_slave_mode = onmaster_slave_sub_mode = &apos;stream&apos;sr_check_period = 5sr_check_user = &apos;repuser&apos;sr_check_password = &apos;xxxx&apos;sr_check_database = &apos;postgres&apos;health_check_period = 10 # Health check period # Disabled (0) by defaulthealth_check_timeout = 20 # Health check timeout # 0 means no timeouthealth_check_user = &apos;pgpool_healthchk&apos; # Health check userhealth_check_password = &apos;xxxx&apos; # Password for health check userhealth_check_database = &apos;postgres&apos;#必须设置，否则primary数据库down了，pgpool不知道，不能及时切换。从库流复制还在连接数据，报连接失败。#只有下次使用pgpool登录时，发现连接不上，然后报错，这时候，才知道挂了，pgpool进行切换。#主备切换的命令行配置#------------------------------------------------------------------------------# FAILOVER AND FAILBACK#------------------------------------------------------------------------------failover_command = &apos;/opt/pgpool/failover_stream.sh %H&apos;#------------------------------------------------------------------------------# WATCHDOG#------------------------------------------------------------------------------# - Enabling -use_watchdog = on# - Watchdog communication Settings -wd_hostname = &apos;master&apos; # Host name or IP address of this watchdog # (change requires restart)wd_port = 9000 # port number for watchdog service # (change requires restart)# - Virtual IP control Setting -delegate_IP = &apos;192.168.1.88&apos; # delegate IP address # If this is empty, virtual IP never bring up. # (change requires restart)if_cmd_path = &apos;/sbin&apos; # path to the directory where if_up/down_cmd # (change requires restart)if_up_cmd = &apos;ifconfig eth0:0 inet $_IP_$ netmask 255.255.255.0&apos; # startup delegate IP command # (change requires restart)if_down_cmd = &apos;ifconfig eth0:0 down&apos; # shutdown delegate IP command # (change requires restart)# -- heartbeat mode --wd_heartbeat_port = 9694 # Port number for receiving heartbeat signal # (change requires restart)wd_heartbeat_keepalive = 2 # Interval time of sending heartbeat signal (sec) # (change requires restart)wd_heartbeat_deadtime = 30 # Deadtime interval for heartbeat signal (sec) # (change requires restart)heartbeat_destination0 = &apos;slave&apos; # Host name or IP address of destination 0 # for sending heartbeat signal. # (change requires restart)heartbeat_destination_port0 = 9694 # Port number of destination 0 for sending # heartbeat signal. Usually this is the # same as wd_heartbeat_port. # (change requires restart)heartbeat_device0 = &apos;eth0&apos; # Name of NIC device (such like &apos;eth0&apos;) # used for sending/receiving heartbeat # signal to/from destination 0. # This works only when this is not empty # and pgpool has root privilege. # (change requires restart)# - Other pgpool Connection Settings -other_pgpool_hostname0 = &apos;slave&apos; # Host name or IP address to connect to for # (change requires restart)other_pgpool_port0 = 9999 # Port number for othet pgpool 0 # (change requires restart)other_wd_port0 = 9000 # Port number for othet watchdog 0 # (change requires restart) slave配置仅列出与master不同的配置项 12heartbeat_destination0 = &apos;master&apos;other_pgpool_hostname0 = &apos;master&apos; 创建failover_stream脚本在master\slave分别操作 1234mkdir /opt/pgpool chown postgres:postgres /opt/pgpoolcd /opt/pgpoolvim failover_stream.sh 12345678910111213#! /bin/sh # Failover command for streaming replication. # Arguments: $1: new master hostname. PGHOME=&apos;/usr/pgsql-10&apos;PGDATA=&apos;/opt/PostgreSQL/data&apos;new_master=$1 trigger_command=&quot;$PGHOME/bin/pg_ctl promote -D $PGDATA&quot; # Prompte standby database. /usr/bin/ssh -T $new_master $trigger_command exit 0; 1chown postgres:postgres failover_stream.sh &amp;&amp;chmod 777 failover_stream.sh 启动创建相关文件夹1234mkdir /var/log/pgpoolchown -R postgres.postgres /var/log/pgpoolmkdir /var/run/pgpoolchown -R postgres.postgres /var/run/pgpool 启动1234# 启动systemctl start pgpool.service# 停止systemctl stop pgpool.service 查看集群状态 12345678910111213[root@172-16-200-15 ~]# psql -h vip -p 9999 -U postgres用户 postgres 的口令：psql (10.4)输入 &quot;help&quot; 来获取帮助信息.postgres=# show pool_nodes; node_id | hostname | port | status | lb_weight | role | select_cnt | load_balance_node | replication_delay ---------+----------+------+--------+-----------+---------+------------+-------------------+------------------- 0 | master | 5432 | up | 0.500000 | primary | 0 | true | 0 1 | slave | 5432 | up | 0.500000 | standby | 0 | false | 0(2 行记录)#在slave上节点也是psql -h vip -p 9999，双pgpool使用虚拟ip，做到高可用。 HA测试模拟pgpool宕机在master节点操作 1234567891011121314# 停止pgpoolsystemctl stop pgpool# 访问[postgres@postgres ~]# psql -h vip -p 9999 -U postgres用户 postgres 的口令：psql (10.4)输入 "help" 来获取帮助信息.postgres=# show pool_nodes; node_id | hostname | port | status | lb_weight | role | select_cnt | load_balance_node | replication_delay ---------+----------+------+--------+-----------+---------+------------+-------------------+------------------- 0 | master | 5432 | up | 0.500000 | primary | 0 | true | 0 1 | slave | 5432 | up | 0.500000 | standby | 0 | false | 0(2 行记录) 模拟PG主库宕机在master操作 12# 停止postgresqlsystemctl stop postgresql-10.service 恢复master节点配置recovery.conf12find / -name &quot;recovery.conf.sample&quot;cp /usr/pgsql-10/share/recovery.conf.sample $PGDATA/recovery.done 然后在recovery.done中添加如下内容 1234recovery_target_timeline = &apos;latest&apos;standby_mode = on # 指明是否开启服务器作为一个备机。在流复制里，这个参数必须要开启。primary_conninfo = &apos;host=salve port=5432 user=repl password=xxxx&apos; # 指明用于备服务器连接到主服务器的连接字符串。 trigger_file = &apos;/opt/PostgreSQL/pg.trigger&apos; # 指定一个触发文件让备服务器感觉到它的时候就会停止流复制（即：故障转移），不要创建这个文件。当你想主从切换的时候才需要创建它。 启动1systemctl start postgresql-10.service 加入节点1pcp_attach_node -d -U postgres -h vip -p 9898 -n 0 参考&emsp;&emsp;1. http://www.pgpool.net/docs/latest/en/html/example-configs.html&emsp;&emsp;2. http://www.pgpool.net/pgpool-web/contrib_docs/watchdog/en.html#more_switch&emsp;&emsp;3. https://www.jianshu.com/p/ef183d0a9213&emsp;&emsp;4. https://www.jianshu.com/p/41d857a5d743]]></content>
      <categories>
        <category>PostgreSQL</category>
      </categories>
      <tags>
        <tag>PostgreSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PostgreSQL的流复制配置]]></title>
    <url>%2F2018%2F05%2F17%2Fpostgresql-streaming-replication%2F</url>
    <content type="text"><![CDATA[基础信息PostgreSQL版本：10.4主库db：master 监听端口5432备库db：slave 监听端口5432 机器信息1234# cat /etc/redhat-releaseCentOS Linux release 7.4.1708 (Core) # uname -aLinux 192-168-1-16 3.10.0-693.5.2.el7.x86_64 #1 SMP Fri Oct 20 20:32:50 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux 配置主库创建复制用户1CREATE USER repl REPLICATION LOGIN CONNECTION LIMIT 2 ENCRYPTED PASSWORD &apos;xxxxx&apos;; 创建.pgpass1234su postgresvim ~/.pgpassslave:5432:replication:repl:xxxxxchmod 600 ~/.pgpass 配置pg_hba.conf增加如下信息 1host replication repl slave trust 配置postgresql.conf主要配置信息 123wal_level = replica # 增加wal归档信息，包括只读服务器需要的信息max_wal_senders = 4 # 流复制的最大连接数wal_keep_segments = 32 # 流复制保留的最大xlog数 重启数据库1systemctl restart postgresql-10.service 配置备库创建.pgpass1234su postgresvim ~/.pgpassmaster:5432:replication:repl:xxxxxchmod 600 ~/.pgpass 使用pg_backendup生成备库-D 为备库的数据目录 12su postgrespg_basebackup -D /opt/PostgreSQL/data -Fp -Xs -v -P -h master -p 5432 -U repl 这时表空间目录和$PGDATA目录已经复制过来了 配置备库recovery.conf首先执行如下命令生成recovery.conf文件 12find / -name &quot;recovery.conf.sample&quot;cp /usr/pgsql-10/share/recovery.conf.sample $PGDATA/recovery.conf 然后在recovery.conf中添加如下内容 1234recovery_target_timeline = &apos;latest&apos;standby_mode = on # 指明是否开启服务器作为一个备机。在流复制里，这个参数必须要开启。primary_conninfo = &apos;host=master port=5432 user=repl password=xxxx&apos; # 指明用于备服务器连接到主服务器的连接字符串。 trigger_file = &apos;/opt/PostgreSQL/pg.trigger&apos; # 指定一个触发文件让备服务器感觉到它的时候就会停止流复制（即：故障转移），不要创建这个文件。当你想主从切换的时候才需要创建它。 启动备库1systemctl start postgresql-10.service 测试主备同步执行如下命令查看备库进程中有”postgres: wal receiver process”，确认主库中进程有”postgres: wal sender process” 1ps -ef | grep postgres 进入测试数据库testdb，主库上执行如下命令返回f，备库上返回t 12345678910111213select pg_is_in_recovery();# 主库testdb=# select pg_is_in_recovery(); pg_is_in_recovery ------------------- f(1 row)# 备库postgres=# select pg_is_in_recovery(); pg_is_in_recovery ------------------- t(1 行记录) 执行如下命令查看快照，它返回主库记录点、备库记录点；主库每增加一条写入，记录点的值就会加1。 12345678910111213testdb=# select txid_current_snapshot(); txid_current_snapshot----------------------- 1820:1820:(1 row)testdb=# insert into ta values(now());INSERT 0 1testdb=# select txid_current_snapshot(); txid_current_snapshot----------------------- 1821:1821:(1 row) 在主库执行如下命令可以查看主备同步状态。 1select * from pg_stat_replication; 字段state显示的同步状态有：startup（连接中）、catchup（同步中）、streaming（同步）；字段sync_state显示的模式有：async（异步）、sync（同步）、potential（虽然现在是异步模式，但是有可能升级到同步模式）主库执行创建数据库 12postgres=# create database testdb;CREATE DATABASE 备库查看数据库已经同步成功 12345678910testdb=# \l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+------------+------------+----------------------- postgres | postgres | UTF8 | zh_CN.UTF8 | zh_CN.UTF8 | template0 | postgres | UTF8 | zh_CN.UTF8 | zh_CN.UTF8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | zh_CN.UTF8 | zh_CN.UTF8 | =c/postgres + | | | | | postgres=CTc/postgres testdb | postgres | UTF8 | zh_CN.UTF8 | zh_CN.UTF8 | 参考&emsp;&emsp;1. https://www.postgresql.org/docs/10/static/warm-standby.html#STREAMING-REPLICATION&emsp;&emsp;2. https://www.mtyun.com/library/postgresql-stream-replication&emsp;&emsp;3. https://dreamer-yzy.github.io/2015/01/24/-翻译-手把手教你配置流复制/&emsp;&emsp;4. https://www.jianshu.com/p/12bc931ebba3]]></content>
      <categories>
        <category>PostgreSQL</category>
      </categories>
      <tags>
        <tag>PostgreSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7下使用RPM安装PostgreSQL10]]></title>
    <url>%2F2018%2F05%2F17%2Fpostgresql-install%2F</url>
    <content type="text"><![CDATA[安装安装依赖1yum -y install libicu libxslt-devel 准备RPM包下载地址 1234postgresql10-10.4-1PGDG.rhel7.x86_64.rpmpostgresql10-contrib-10.4-1PGDG.rhel7.x86_64.rpmpostgresql10-libs-10.4-1PGDG.rhel7.x86_64.rpmpostgresql10-server-10.4-1PGDG.rhel7.x86_64.rpm 安装1rpm -ivh *.rpm 初始化数据库默认数据目录： /var/lib/pgsql/10/data/ 1/usr/pgsql-10/bin/postgresql-10-setup initdb 自定义目录 12345mkdir -p /opt/PostgreSQL/datachmod 755 -R /opt/PostgreSQL/datachown postgres:postgres -R /opt/PostgreSQLsu postgres/usr/pgsql-10/bin/initdb --encoding=UTF-8 --local=zh_CN.UTF8 --username=postgres --pwprompt --pgdata=/opt/PostgreSQL/data 修改service环境变量修改Environment:PGDATA的数据目录为/opt/PostgreSQL/data 12vim /usr/lib/systemd/system/postgresql-10.servicesystemctl daemon-reload 修改postgres用户的环境变量PGDATA 12su postgresvi ~/.bash_profile 启动123456# 启动systemctl start postgresql-10.service# 开机启动systemctl enable postgresql-10.service# 关闭开机启动systemctl disable postgresql-10.service 登录12su postgrespsql -U postgres 查看当前数据目录1234show data_directory;---------------------- /opt/PostgreSQL/data(1 row) 修改密码12postgres=# \passwordALTER USER postgres WITH PASSWORD &apos;xxxxx&apos;; 数据库登录权限设置$PGDATA/pg_hba.conf 权限相关配置 12345678910111213141516# TYPE DATABASE USER ADDRESS METHOD# &quot;local&quot; is for Unix domain socket connections onlylocal all all md5# IPv4 local connections:#host all all 127.0.0.1/32 trusthost all all 0.0.0.0/0 md5# IPv6 local connections:#host all all ::1/128 trusthost all all ::0/0 md5# Allow replication connections from localhost, by a user with the# replication privilege.#local replication all trust#host replication all 127.0.0.1/32 trust#host replication all ::1/128 trusthost replication repl slave trust 设置trust，本地可以使用psql -U postgres直接登录服务器设置peer，本地可以使用psql -h 127.0.0.1 -d postgres -U postgres直接登录服务器设置password，使用用户名密码登录 $PGDATA/postgresql.conf 数据库相关配置 12listen_addresses = &apos;*&apos;posrt = 5432 设置监听任意IP, 允许任意ip连接数据库更多权限说明，见官方文档]]></content>
      <categories>
        <category>PostgreSQL</category>
      </categories>
      <tags>
        <tag>PostgreSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Python脚本下载阿里云rds实例备份，上传至oss]]></title>
    <url>%2F2018%2F04%2F17%2Faliyun-rds-backup%2F</url>
    <content type="text"><![CDATA[价格对比 RDS备份空间费用信息计费量＝数据备份量（OSS＋OAS）＋日志备份量（OSS）－ 50%*实例购买的存储空间（单位为 GB，只入不舍），价格为 0.001 元/GB/小时。 OSS归档型单价归档型存储，Object最短存储期限为60天，早于60天删除、修改、覆盖Object，需要补足未满60天的剩余天数的存储费用，超过60天不需要补。价格为0.033/GB/月 环境准备安装sdk安装对应sdk，查看sdk列表以下脚本依赖的sdk版本 1234aliyun-python-sdk-core (2.3.5)aliyun-python-sdk-core-v3 (2.5.2)aliyun-python-sdk-rds (2.1.1)oss2 (2.4.0) 申请Access Key新建RAM账号，创建ak 脚本aliyun_api_core.py认证模块 12345678910111213141516171819202122232425262728293031323334# -*- coding: UTF-8 -*-import oss2import jsonfrom aliyunsdkcore import clientclass aliyun_api_core(object): def __init__(self): self.clt = client.AcsClient( ak="", secret="") self.ossauth = oss2.Auth( access_key_id="", access_key_secret="") # oss访问地址 self.endpoint = 'http://oss-cn-hangzhou-internal.aliyuncs.com' # bucket信息 self.bucket = oss2.Bucket(self.ossauth, self.endpoint, "bucket_name") def request_api(self, request, *values): if values: for value in values: for k, v in value.items(): request.add_query_param(k, v) request.set_accept_format('json') result = self.clt.do_action_with_exception(request) return json.dumps(json.loads(result.decode('utf-8')), indent=4, sort_keys=False, ensure_ascii=False)if __name__ == '__main__': t = aliyun_api_core() aliyun_api_oss.pyoss上传相关 12345678910111213141516171819202122232425262728# -*- coding: UTF-8 -*-import oss2from aliyun_api.aliyun_api_core import aliyun_api_corecore = aliyun_api_core()class aliyun_api_oss(object): def __init__(self): self.ossauth = core.ossauth self.endpoint = core.endpoint self.bucket = core.bucket def multithread_upload(self, backup_file_name, backup_file_path): '''多线程上传，自动分片''' key = backup_file_name pathname = backup_file_path # 判断文件是否存在，已存在的跳过 exist = self.bucket.object_exists(backup_file_name) if exist: pass else: oss2.resumable_upload(self.bucket, key, pathname, multipart_threshold=64 * 1024 * 1024, num_threads=4)if __name__ == '__main__': t = aliyun_api_oss() aliyun_api_rds.pyrds备份文件查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# -*- coding: UTF-8 -*-import jsonfrom aliyun_api.aliyun_api_core import aliyun_api_corefrom aliyunsdkrds.request.v20140815 import DescribeRegionsRequest, DescribeBackupsRequest, DescribeDBInstancesRequest, \ DescribeDBInstanceAttributeRequest, DescribeBackupPolicyRequest, DescribeBinlogFilesRequestcore = aliyun_api_core()class aliyun_api_rds(object): def __init__(self): self.clt = core.clt self.request_api = core.request_api def DescribeRegions(self, **kwargs): '''查询RDS地域和可用区信息''' request = DescribeRegionsRequest.DescribeRegionsRequest() values = &#123;"action_name": "DescribeRegions"&#125; values = dict(values, **kwargs) result = self.request_api(request, values) return result def DescribeDBInstances(self, RegionId="cn-hangzhou", **kwargs): '''查看RDS实例列表''' request = DescribeDBInstancesRequest.DescribeDBInstancesRequest() values = &#123;"action_name": "DescribeDBInstances", "RegionId": RegionId, "PageSize": 100&#125; values = dict(values, **kwargs) result = self.request_api(request, values) return result def DescribeDBInstanceAttribute(self, DBInstanceId, **kwargs): '''查看RDS实例详情''' request = DescribeDBInstanceAttributeRequest.DescribeDBInstanceAttributeRequest() values = &#123;"action_name": "DescribeDBInstanceAttribute", "DBInstanceId": DBInstanceId&#125; values = dict(values, **kwargs) result = self.request_api(request, values) return result def DescribeBackups(self, DBInstanceId, StartTime, EndTime, BackupStatus="Success", **kwargs): '''查看RDS实例备份列表''' request = DescribeBackupsRequest.DescribeBackupsRequest() values = &#123;"action_name": "DescribeBackups", "DBInstanceId": DBInstanceId, "StartTime": StartTime, "EndTime": EndTime, "BackupStatus": BackupStatus&#125; values = dict(values, **kwargs) result = self.request_api(request, values) return result def DescribeBinlogFiles(self, DBInstanceId, StartTime, EndTime, **kwargs): '''查看RDS实例BINLOG备份列表''' request = DescribeBinlogFilesRequest.DescribeBinlogFilesRequest() values = &#123;"action_name": "DescribeBinlogFiles", "DBInstanceId": DBInstanceId, "StartTime": StartTime, "EndTime": EndTime, "PageSize": 100&#125; values = dict(values, **kwargs) result = self.request_api(request, values) return resultif __name__ == '__main__': t = aliyun_api_rds() backup.py备份主模块 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208# -*- coding:UTF-8 -*-import osimport sysimport jsonimport datetimeimport urllib.requestimport loggingimport logging.configfrom aliyun_api.aliyun_api_rds import aliyun_api_rdsfrom aliyun_api.aliyun_api_oss import aliyun_api_ossconf_filepath = os.path.join(os.path.dirname(__file__), 'log/logging.conf')logging.config.fileConfig(conf_filepath)api_rds = aliyun_api_rds()api_oss = aliyun_api_oss()# 本地路径basedir = "/aliyun_backup/"basedir_rds = os.path.join(basedir, 'RDS/')basedir_rds_binlog = os.path.join(basedir, 'RDS/BINLOG/')# 目标目录（在Oss中的目录）ossdir_rds = "RDS/"ossdir_rds_binlog = "RDS/BINLOG/"# RDS区域id sys.argv[1]RegionId = ""today = datetime.date.today()# 起止时间为前一天的0点到24点StartTime = '%sT16:00Z' % (today - datetime.timedelta(days=2))EndTime = '%sT15:59Z' % (today - datetime.timedelta(days=1))BinLog_StartTime = '%sT16:00:00Z' % (today - datetime.timedelta(days=2))BinLog_EndTime = '%sT15:59:59Z' % (today - datetime.timedelta(days=1))def report_hook(count, block_size, total_size): ''' 下载进度显示回调函数 @count:已经下载的数据块 @block_size:数据块的大小 @total_size:远程文件的大小 ''' per = 100.0 * count * block_size / total_size logging.info("%.2f%%" % (100.0 * count * block_size / total_size))def time_format(str_time): ''' 阿里云2017-12-10T16:00:00Z时间格式上加上8小时时区 ''' Ymd = str_time.split('T')[0] HMS = str_time.split('T')[1].split('Z')[0] str_time = '%s %s' % (Ymd, HMS) time = datetime.datetime.strptime(str_time, "%Y-%m-%d %H:%M:%S") format_time = time + datetime.timedelta(hours=8) return format_timedef download_backup_file(basedir, ossdir, DBInstanceId, backup_file_url, **kwargs): ''' 下载备份文件 ''' # 此处不替换貌似无法下载 if 'i-internal' in backup_file_url: backup_file_url = backup_file_url.replace('i-internal', 'internal') # 拼接文件名，否则BINLOG无法知道时间范围，不能增量还原 if basedir == basedir_rds_binlog: LogBeginTime = time_format(kwargs['LogBeginTime']).strftime("%Y%m%d%H%M%S") LogEndTime = time_format(kwargs['LogEndTime']).strftime("%Y%m%d%H%M%S") backup_file_name = DBInstanceId + '_hins' + str(kwargs['HostInstanceID']) + '_' + LogBeginTime + '_' + LogEndTime + '_' + backup_file_url.rsplit('/', 1)[1].split('?')[0] else: backup_file_name = DBInstanceId + '_' + backup_file_url.rsplit('/', 1)[1].split('?')[0] backup_file_dir = os.path.abspath(basedir + DBInstanceId) logging.info(backup_file_url) # 判断目录是否存在，不存在创建下载路径 if not os.path.exists(backup_file_dir): os.makedirs(backup_file_dir) backup_file_local_path = os.path.join(backup_file_dir, backup_file_name) backup_file_remote_path = ossdir + DBInstanceId + '/' + backup_file_name urllib.request.urlretrieve(backup_file_url, backup_file_local_path) return backup_file_local_path, backup_file_remote_pathclass backup_rds_cl(object): def backup_rds(self, DBInstanceId, StartTime, EndTime): ''' 备份指定实例ID的备份数据到OSS ''' # 获取RDS详情 DBInstance = json.loads(api_rds.DescribeDBInstanceAttribute(DBInstanceId))['Items']['DBInstanceAttribute'][0] # 获取RDS实例的备份列表 DBInstanceBackupList = json.loads(api_rds.DescribeBackups(DBInstanceId, StartTime, EndTime)) # 遍历所有RDS备份 DBInstanceBackupCount = DBInstanceBackupList['TotalRecordCount'] # 时间范围内实例备份数 logging.info("(%s)实例id:%s,从%s到%s,共有%d个备份文件可以下载" % ( DBInstance['DBInstanceDescription'], DBInstance['DBInstanceId'], StartTime, EndTime, DBInstanceBackupCount)) if DBInstanceBackupCount &gt; 0: i = 0 for DBInstanceBackup in DBInstanceBackupList['Items']['Backup']: i = i + 1 logging.info("(%s)实例id:%s,正在下载第%d个备份文件,请稍候..." % ( DBInstance['DBInstanceDescription'], DBInstance['DBInstanceId'], i)) backup_file_url = DBInstanceBackup['BackupIntranetDownloadURL'] # 内网下载地址 BackupDownloadURL # 下载文件到本地 ossdir = ossdir_rds backup_file_local_path, backup_file_remote_path = \ download_backup_file(basedir_rds, ossdir, DBInstance['DBInstanceId'], backup_file_url) logging.info("(%s)实例id:%s,第%d个备份文件下载完成,开始上传到OSS" % ( DBInstance['DBInstanceDescription'], DBInstance['DBInstanceId'], i)) # 上传到OSS api_oss.multithread_upload(backup_file_remote_path, backup_file_local_path) logging.info( "(%s)实例id:%s,第%d个备份文件上传完成" % (DBInstance['DBInstanceDescription'], DBInstance['DBInstanceId'], i)) # 删除上传成功的文件 os.remove(backup_file_local_path) logging.info("(%s)实例id:%s,所有备份上传完成！！！" % (DBInstance['DBInstanceDescription'], DBInstance['DBInstanceId'])) def backup_rds_binlog(self, DBInstanceId, StartTime, EndTime): ''' 备份指定实例ID的BINLOG备份数据到OSS ''' # 获取RDS详情 DBInstance = json.loads(api_rds.DescribeDBInstanceAttribute(DBInstanceId))['Items']['DBInstanceAttribute'][0] # 获取RDS实例的binlog备份列表 DBInstanceBinlogBackupList = json.loads(api_rds.DescribeBinlogFiles(DBInstanceId, StartTime, EndTime)) # 遍历所有RDS备份 DBInstanceBackupCount = DBInstanceBinlogBackupList['TotalRecordCount'] # 时间范围内实例备份数 logging.info("(%s)实例id:%s,从%s到%s,共有%d个BINLOG备份文件可以下载" % ( DBInstance['DBInstanceDescription'], DBInstance['DBInstanceId'], StartTime, EndTime, DBInstanceBackupCount)) if DBInstanceBackupCount &gt; 0: i = 0 for DBInstanceBackup in DBInstanceBinlogBackupList['Items']['BinLogFile']: i = i + 1 logging.info("(%s)实例id:%s,正在下载第%d个BINLOG备份文件,请稍候..." % ( DBInstance['DBInstanceDescription'], DBInstance['DBInstanceId'], i)) backup_file_url = DBInstanceBackup['IntranetDownloadLink'] # 内网下载地址 DownloadLink\IntranetDownloadLink # 下载文件到本地 backup_file_local_path, backup_file_remote_path = \ download_backup_file(basedir_rds_binlog, ossdir_rds_binlog, DBInstance['DBInstanceId'], backup_file_url, HostInstanceID=DBInstanceBackup['HostInstanceID'], LogBeginTime=DBInstanceBackup['LogBeginTime'], LogEndTime=DBInstanceBackup['LogEndTime']) logging.info("(%s)实例id:%s,第%d个BINLOG备份文件下载完成,开始上传到OSS" % ( DBInstance['DBInstanceDescription'], DBInstance['DBInstanceId'], i)) # 上传到OSS api_oss.multithread_upload(backup_file_remote_path, backup_file_local_path) logging.info( "(%s)实例id:%s,第%d个BINLOG备份文件上传完成" % (DBInstance['DBInstanceDescription'], DBInstance['DBInstanceId'], i)) # 删除上传成功的文件 os.remove(backup_file_local_path) logging.info("(%s)实例id:%s,所有BINLOG备份上传完成！！！" % (DBInstance['DBInstanceDescription'], DBInstance['DBInstanceId'])) def backup_all_rds(self, RegionId, StartTime, EndTime): ''' 备份全部实例ID的备份数据到OSS ''' # 获取RDS实例列表 DescribeDBInstances = json.loads(api_rds.DescribeDBInstances(RegionId)) # 遍历所有RDS实例 DBInstanceCount = DescribeDBInstances['TotalRecordCount'] # 区域内实例数，如果多区域可遍历 if DBInstanceCount &gt; 0: for DBInstance in DescribeDBInstances['Items']['DBInstance']: self.backup_rds(DBInstance['DBInstanceId'], StartTime, EndTime) logging.info("所有实例的备份全部上传完成！！！") def backup_allbinlog_rds(self, RegionId, StartTime, EndTime): ''' 备份全部实例ID的BILONG备份数据到OSS ''' # 获取RDS实例列表 DescribeDBInstances = json.loads(api_rds.DescribeDBInstances(RegionId, Engine='MySQL')) # 遍历所有RDS实例 DBInstanceCount = DescribeDBInstances['TotalRecordCount'] # 区域内实例数，如果多区域可遍历 if DBInstanceCount &gt; 0: for DBInstance in DescribeDBInstances['Items']['DBInstance']: self.backup_rds_binlog(DBInstance['DBInstanceId'], StartTime, EndTime) logging.info("所有实例的BINLOG备份全部上传完成！！！")if __name__ == '__main__': # 备份全部rds实例 rds = backup_rds_cl() logging.info("=================开始上传rds实例备份================") rds.backup_all_rds(RegionId, StartTime, EndTime) logging.info("=================开始上传rds实例BINLOG备份================") rds.backup_allbinlog_rds(RegionId, BinLog_StartTime, BinLog_EndTime) logging.conflog配置文件，记录备份详细情况 12345678910111213141516171819202122232425262728293031# logging.conf[loggers]keys=root[logger_root]level=INFOhandlers=consoleHandler,fileHandler#################################################[handlers]keys=consoleHandler,fileHandler[handler_consoleHandler]class=StreamHandlerlevel=INFOformatter=simpleFormatterargs=(sys.stdout,)[handler_fileHandler]class=FileHandlerlevel=INFOformatter=simpleFormatterargs=('log/logger.log','a','utf-8')#################################################[formatters]keys=simpleFormatter[formatter_simpleFormatter]format=%(asctime)s %(filename)s : %(levelname)s %(message)sdatefmt=%Y-%m-%d %A %H:%M:%S]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>rds</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python调用阿里云sdk]]></title>
    <url>%2F2018%2F04%2F17%2Faliyun-python-sdk-rds%2F</url>
    <content type="text"><![CDATA[准备安装sdk阿里云封装了各个产品的api，方便集成开发，sdk列表 1pip install aliyun-python-sdk-rds 申请Access Key新建RAM账号，创建ak 脚本编写下面是archer获取慢日志相关的脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# -*- coding: UTF-8 -*-import datetimefrom aliyunsdkcore import clientfrom aliyunsdkrds.request.v20140815 import DescribeSlowLogsRequest, DescribeSlowLogRecordsRequest, \ RequestServiceOfCloudDBARequestimport simplejson as jsonclass Aliyun(object): # ak认证 def __init__(self): self.clt = client.AcsClient( ak=ak, secret=secret) # 调用方法 def request_api(self, request, *values): if values: for value in values: for k, v in value.items(): request.add_query_param(k, v) request.set_accept_format('json') result = self.clt.do_action_with_exception(request) return json.dumps(json.loads(result.decode('utf-8')), indent=4, sort_keys=False, ensure_ascii=False) def DescribeSlowLogs(self, DBInstanceId, StartTime, EndTime, **kwargs): ''' 获取集群慢日志列表 DBName,SortKey、PageSize、PageNumber ''' request = DescribeSlowLogsRequest.DescribeSlowLogsRequest() values = &#123;"action_name": "DescribeSlowLogs", "DBInstanceId": DBInstanceId, "StartTime": StartTime, "EndTime": EndTime, "SortKey": "TotalExecutionCounts"&#125; values = dict(values, **kwargs) result = self.request_api(request, values) return result def DescribeSlowLogRecords(self, DBInstanceId, StartTime, EndTime, **kwargs): ''' 查看慢日志明细 SQLId,DBName、PageSize、PageNumber ''' request = DescribeSlowLogRecordsRequest.DescribeSlowLogRecordsRequest() values = &#123;"action_name": "DescribeSlowLogRecords", "DBInstanceId": DBInstanceId, "StartTime": StartTime, "EndTime": EndTime&#125; values = dict(values, **kwargs) result = self.request_api(request, values) return result def RequestServiceOfCloudDBA(self, DBInstanceId, ServiceRequestType, ServiceRequestParam, **kwargs): ''' 获取统计信息：'GetTimedMonData',&#123;"Language":"zh","KeyGroup":"mem_cpu_usage","KeyName":"","StartTime":"2018-01-15T04:03:26Z","EndTime":"2018-01-15T05:03:26Z"&#125; mem_cpu_usage、iops_usage、detailed_disk_space 获取process信息：'ShowProcessList',&#123;"Language":"zh","Command":"Query"&#125; -- Not Sleep , All 终止进程：'ConfirmKillSessionRequest',&#123;"Language":"zh","SQLRequestID":75865,"SQLStatement":"kill 34022786;"&#125; 获取表空间信息：'GetSpaceStatForTables',&#123;"Language": "zh", "OrderType": "Data"&#125; 获取资源利用信息：'GetResourceUsage',&#123;"Language":"zh"&#125; ''' request = RequestServiceOfCloudDBARequest.RequestServiceOfCloudDBARequest() values = &#123;"action_name": "RequestServiceOfCloudDBA", "DBInstanceId": DBInstanceId, "ServiceRequestType": ServiceRequestType, "ServiceRequestParam": ServiceRequestParam&#125; values = dict(values, **kwargs) result = self.request_api(request, values) return result]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>rds</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Data Masking 动态数据脱敏]]></title>
    <url>%2F2018%2F04%2F16%2Fdata-masking%2F</url>
    <content type="text"><![CDATA[实现思路 解析查询语句 脱敏规则验证 改写SQL/对结果脱敏 具体实现解析查询语句主要有两个选择阿里的druid，作为数据库连接池，它自身也有一个SQL解析模块，开源的分布式数据库中间件mycat就是基于该模块实现的语句解析。去哪儿的inception，自身有一个语法树打印的功能，能够解析查询语句，最终输出为可视化的query_tree。 druid demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.tops001.sqlparser.durid.sqlparser;import com.alibaba.druid.sql.ast.SQLStatement;import com.alibaba.druid.sql.ast.statement.SQLSelect;import com.alibaba.druid.sql.ast.statement.SQLSelectQueryBlock;import com.alibaba.druid.sql.ast.statement.SQLSelectStatement;import com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser;import com.alibaba.druid.sql.dialect.mysql.visitor.MySqlSchemaStatVisitor;public class ParseTest &#123; public static void main(String[] args) &#123; String sql = "SELECT\n" + " a.id,\n" + " username,\n" + " b.workflow_name,\n" + " reviewok_time\n" + "FROM archer.sql_users a\n" + " JOIN sql_workflow b ON a.username = b.engineer;\n"; // 新建 MySQL Parser MySqlStatementParser parser = new MySqlStatementParser(sql); // 使用Parser解析生成AST，这里SQLStatement就是AST SQLStatement statement = parser.parseStatement(); // 使用visitor来访问AST MySqlSchemaStatVisitor visitor = new MySqlSchemaStatVisitor(); statement.accept(visitor); // 从visitor中拿出你所关注的信息 System.out.println(visitor.getTables()); System.out.println(visitor.getColumns()); //使用mysql解析 MySqlStatementParser mySqlStatementParser = new MySqlStatementParser(sql) ; //解析select查询 SQLSelectStatement sqlSelectStatement = (SQLSelectStatement) mySqlStatementParser.parseSelect(); SQLSelect sqlSelect = sqlSelectStatement.getSelect() ; //获取sql查询块 SQLSelectQueryBlock sqlSelectQueryBlock = (SQLSelectQueryBlock)sqlSelect.getQuery() ; System.out.println(sqlSelectQueryBlock.getSelectList()); &#125;&#125; 解析结果 123&#123;archer.sql_users=Select, sql_workflow=Select&#125;[archer.sql_users.username, sql_workflow.engineer, archer.sql_users.id, UNKNOWN.username, sql_workflow.workflow_name, UNKNOWN.reviewok_time][a.id, username, b.workflow_name, reviewok_time] inception demo12345678910111213141516171819202122232425262728293031323334353637#!/usr/bin/python# coding: utf-8import MySQLdbimport jsonsqlcontent = '''use archer; SELECT a.id, username, b.workflow_name, reviewok_timeFROM archer.sql_users a JOIN sql_workflow b ON a.username = b.engineer;'''sql = "/*--user=;--password=;--host=;--port=;--enable-query-print;*/ \inception_magic_start;\%s\inception_magic_commit;" % sqlcontenttry: conn = MySQLdb.connect(host='', user='', passwd='', db='', port=6999) cur = conn.cursor() ret = cur.execute(sql) result = cur.fetchall() num_fields = len(cur.description) field_names = [i[0] for i in cur.description] print (field_names) for row in result: print ( row[0], "|", row[1], "|", row[2], "|", row[3], "|", row[4]) # print ( # row[0], "|", row[1], "|", row[2], "|", row[3], "|", row[4], "|", row[5], "|", row[6], "|", row[7], "|", # row[8], "|", row[9], "|", row[10]) cur.close() conn.close()except MySQLdb.Error as e: print ("Mysql Error %d: %s" % (e.args[0], e.args[1])) 解析结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; &quot;command&quot;: &quot;select&quot;, &quot;select_list&quot;: [&#123; &quot;type&quot;: &quot;FIELD_ITEM&quot;, &quot;db&quot;: &quot;archer&quot;, &quot;table&quot;: &quot;sql_users&quot;, &quot;field&quot;: &quot;id&quot; &#125;, &#123; &quot;type&quot;: &quot;FIELD_ITEM&quot;, &quot;db&quot;: &quot;archer&quot;, &quot;table&quot;: &quot;sql_users&quot;, &quot;field&quot;: &quot;username&quot; &#125;, &#123; &quot;type&quot;: &quot;FIELD_ITEM&quot;, &quot;db&quot;: &quot;archer&quot;, &quot;table&quot;: &quot;sql_workflow&quot;, &quot;field&quot;: &quot;workflow_name&quot; &#125;, &#123; &quot;type&quot;: &quot;FIELD_ITEM&quot;, &quot;db&quot;: &quot;archer&quot;, &quot;table&quot;: &quot;sql_workflow&quot;, &quot;field&quot;: &quot;reviewok_time&quot; &#125;], &quot;table_ref&quot;: [&#123; &quot;db&quot;: &quot;archer&quot;, &quot;table&quot;: &quot;sql_users&quot; &#125;, &#123; &quot;db&quot;: &quot;archer&quot;, &quot;table&quot;: &quot;sql_workflow&quot; &#125;], &quot;join_on&quot;: [&#123; &quot;type&quot;: &quot;FUNC_ITEM&quot;, &quot;func&quot;: &quot;=&quot;, &quot;args&quot;: [&#123; &quot;type&quot;: &quot;FIELD_ITEM&quot;, &quot;db&quot;: &quot;archer&quot;, &quot;table&quot;: &quot;sql_users&quot;, &quot;field&quot;: &quot;username&quot; &#125;, &#123; &quot;type&quot;: &quot;FIELD_ITEM&quot;, &quot;db&quot;: &quot;archer&quot;, &quot;table&quot;: &quot;sql_workflow&quot;, &quot;field&quot;: &quot;engineer&quot; &#125;] &#125;]&#125; 由于archer本身是基于inception做的SQL上线审核，最后SQL查询解析模块也顺势选择了inception，但是inception有一些已知的无法解析的语法，如子查询嵌套，但用于简单查询已足够。 脱敏规则脱敏规则定义表12345678910CREATE TABLE `data_masking_rules` ( `id` int(11) NOT NULL AUTO_INCREMENT, `rule_type` TINYINT NOT NULL DEFAULT 0 COMMENT &apos;规则类型，1手机号，2姓名，3证件号码，4银行卡，5邮箱&apos;, `rule_regex` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT&apos;规则脱敏所用的正则表达式，表达式必须分组，隐藏的组会使用****代替&apos;, `hide_group` TINYINT NOT NULL DEFAULT 0 COMMENT&apos;需要隐藏的组&apos;, `rule_desc` varchar(100) NOT NULL DEFAULT &apos;规则描述&apos;, `sys_time` TIMESTAMP NOT NULL DEFAULT current_timestamp COMMENT &apos;修改时间&apos;, PRIMARY KEY (`id`), UNIQUE KEY `rule_type` (`rule_type`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 脱敏字段定义表12345678910111213CREATE TABLE `data_masking_columns` ( `column_id` int(11) NOT NULL AUTO_INCREMENT, `rule_type` TINYINT NOT NULL DEFAULT 0 COMMENT &apos;规则类型，1手机号，2姓名，3证件号码，4银行卡，5邮箱&apos;, `active` TINYINT DEFAULT 0 COMMENT &apos;激活状态，0未激活，1激活&apos;, `table_schema` varchar(64) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;字段所在库名&apos;, `table_name` varchar(64) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;字段所在表名&apos;, `column_name` varchar(64) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;字段名&apos;, `column_comment` varchar(1024) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;字段描述&apos;, `create_time` TIMESTAMP NOT NULL DEFAULT current_timestamp COMMENT &apos;创建时间&apos;, `sys_time` TIMESTAMP NOT NULL DEFAULT current_timestamp COMMENT &apos;修改时间&apos;, `cluster_name` varchar(50) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;字段所在集群&apos;, PRIMARY KEY (`column_id`),) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 数据脱敏改写SQL123456789101112131415手机号：保留前三后四，中间用*代替 SELECT insert(phone, 4, 4, &apos;****&apos;), phoneFROM user WHERE phone&lt;&gt;&apos;&apos;LIMIT 100;邮箱：去除后缀 SELECT substring_index(email, &apos;@&apos;, 1), emailFROM user WHERE email&lt;&gt;&apos;&apos;LIMIT 100; 正则处理结果集123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267# -*- coding:utf-8 -*-from .inception import InceptionDaofrom .models import DataMaskingRules, DataMaskingColumnsimport jsonimport reinceptionDao = InceptionDao()class Masking(object): # 脱敏数据 def data_masking(self, cluster_name, db_name, sql, sql_result): result = &#123;'status': 0, 'msg': 'ok', 'data': []&#125; # 通过inception获取语法树,并进行解析 print_info = self.query_tree(sql, cluster_name, db_name) if print_info is None: result['status'] = 1 result['msg'] = 'inception返回的结果集为空！可能是SQL语句有语法错误' elif print_info['errlevel'] != 0: result['status'] = 2 result['msg'] = 'inception返回异常：\n' + print_info['errmsg'] else: query_tree = print_info['query_tree'] # 获取集群所属环境,获取命中脱敏规则的列数据 table_hit_columns, hit_columns = self.analy_query_tree(query_tree, cluster_name) # 存在select * 的查询,遍历column_list,获取命中列的index,添加到hit_columns if table_hit_columns and sql_result.get('rows'): column_list = sql_result['column_list'] table_hit_column = &#123;&#125; for column_info in table_hit_columns: table_hit_column_info = &#123;&#125; rule_type = column_info['rule_type'] table_hit_column_info[column_info['column_name']] = rule_type table_hit_column.update(table_hit_column_info) for index, item in enumerate(column_list): if item in table_hit_column.keys(): column = &#123;&#125; column['column_name'] = item column['index'] = index column['rule_type'] = table_hit_column.get(item) hit_columns.append(column) # 对命中规则列hit_columns的数据进行脱敏 # 获取全部脱敏规则信息，减少循环查询，提升效率 DataMaskingRulesOb = DataMaskingRules.objects.all() if hit_columns and sql_result.get('rows'): rows = list(sql_result['rows']) for column in hit_columns: index = column['index'] for idx, item in enumerate(rows): rows[idx] = list(item) rows[idx][index] = self.regex(DataMaskingRulesOb, column['rule_type'], rows[idx][index]) sql_result['rows'] = rows return result # 通过inception获取语法树 def query_tree(self, sqlContent, cluster_name, dbName): print_info = inceptionDao.query_print(sqlContent, cluster_name, dbName) if print_info: id = print_info[0][0] statement = print_info[0][1] # 返回值为非0的情况下，说明是有错的，1表示警告，不影响执行，2表示严重错误，必须修改 errlevel = print_info[0][2] query_tree = print_info[0][3] errmsg = print_info[0][4] # 提交给inception语法错误的情况 if errmsg == 'Global environment': errlevel = 2 errmsg = 'Global environment: ' + query_tree if errlevel == 0: print(json.dumps(json.loads(query_tree), indent=4, sort_keys=False, ensure_ascii=False)) return &#123;'id': id, 'statement': statement, 'errlevel': errlevel, 'query_tree': query_tree, 'errmsg': errmsg&#125; else: return None # 解析语法树，获取语句涉及的表，用于查询权限限制 def query_table_ref(self, sqlContent, cluster_name, dbName): result = &#123;'status': 0, 'msg': 'ok', 'data': []&#125; print_info = self.query_tree(sqlContent, cluster_name, dbName) if print_info is None: result['status'] = 1 result['msg'] = 'inception返回的结果集为空！可能是SQL语句有语法错误' elif print_info['errlevel'] != 0: result['status'] = 2 result['msg'] = 'inception返回异常：\n' + print_info['errmsg'] else: table_ref = json.loads(print_info['query_tree'])['table_ref'] result['data'] = table_ref return result # 解析query_tree,获取语句信息,并返回命中脱敏规则的列信息 def analy_query_tree(self, query_tree, cluster_name): query_tree_dict = json.loads(query_tree) select_list = query_tree_dict.get('select_list') table_ref = query_tree_dict.get('table_ref') # 获取全部脱敏字段信息，减少循环查询，提升效率 DataMaskingColumnsOb = DataMaskingColumns.objects.all() # 遍历select_list columns = [] hit_columns = [] # 命中列 table_hit_columns = [] # 涉及表命中的列 # 获取select信息的规则，仅处理type为FIELD_ITEM的select信息，如[*],[*,column_a],[column_a,*],[column_a,a.*,column_b],[a.*,column_a,b.*], select_index = [select_item['field'] for select_item in select_list if select_item['type'] == 'FIELD_ITEM'] if select_index: # 如果发现存在field='*',则遍历所有表,找出所有的命中字段 if '*' in select_index: for table in table_ref: hit_columns_info = self.hit_table(DataMaskingColumnsOb, cluster_name, table['db'], table['table']) table_hit_columns.extend(hit_columns_info) # [*] if re.match(r"^(\*,?)+$", ','.join(select_index)): hit_columns = [] # [*,column_a] elif re.match(r"^(\*,)+(\w,?)+$", ','.join(select_index)): # 找出field不为* 的列信息, 循环判断列是否命中脱敏规则，并增加规则类型和index,index采取后切片 for index, item in enumerate(select_list): if item['type'] == 'FIELD_ITEM': item['index'] = index - len(select_list) if item['field'] != '*': columns.append(item) for column in columns: hit_info = self.hit_column(DataMaskingColumnsOb, cluster_name, column['db'], column['table'], column['field']) if hit_info['is_hit']: hit_info['index'] = column['index'] hit_columns.append(hit_info) # [column_a, *] elif re.match(r"^(\w,?)+(\*,?)+$", ','.join(select_index)): # 找出field不为* 的列信息, 循环判断列是否命中脱敏规则，并增加规则类型和index,index采取前切片 for index, item in enumerate(select_list): if item['type'] == 'FIELD_ITEM': item['index'] = index if item['field'] != '*': columns.append(item) for column in columns: hit_info = self.hit_column(DataMaskingColumnsOb, cluster_name, column['db'], column['table'], column['field']) if hit_info['is_hit']: hit_info['index'] = column['index'] hit_columns.append(hit_info) # [column_a,a.*,column_b] elif re.match(r"^(\w,?)+(\*,?)+(\w,?)+$", ','.join(select_index)): # 找出field不为* 的列信息, 循环判断列是否命中脱敏规则，并增加规则类型和index,*前面的字段index采取前切片,*后面的字段采取后切片 for index, item in enumerate(select_list): if item['type'] == 'FIELD_ITEM': item['index'] = index if item['field'] == '*': first_idx = index break select_list.reverse() for index, item in enumerate(select_list): if item['type'] == 'FIELD_ITEM': item['index'] = index if item['field'] == '*': last_idx = len(select_list) - index - 1 break select_list.reverse() for index, item in enumerate(select_list): if item['type'] == 'FIELD_ITEM': if item['field'] != '*' and index &lt; first_idx: item['index'] = index columns.append(item) if item['field'] != '*' and index &gt; last_idx: item['index'] = index - len(select_list) columns.append(item) for column in columns: hit_info = self.hit_column(DataMaskingColumnsOb, cluster_name, column['db'], column['table'], column['field']) if hit_info['is_hit']: hit_info['index'] = column['index'] hit_columns.append(hit_info) # [a.*, column_a, b.*] else: hit_columns = [] return table_hit_columns, hit_columns # 没有*的查询，直接遍历查询命中字段，query_tree的列index就是查询语句列的index else: for index, item in enumerate(select_list): if item['type'] == 'FIELD_ITEM': item['index'] = index if item['field'] != '*': columns.append(item) for column in columns: hit_info = self.hit_column(DataMaskingColumnsOb, cluster_name, column['db'], column['table'], column['field']) if hit_info['is_hit']: hit_info['index'] = column['index'] hit_columns.append(hit_info) return table_hit_columns, hit_columns # 判断字段是否命中脱敏规则,如果命中则返回脱敏的规则id和规则类型 def hit_column(self, DataMaskingColumnsOb, cluster_name, table_schema, table_name, column_name): column_info = DataMaskingColumnsOb.filter(cluster_name=cluster_name, table_schema=table_schema, table_name=table_name, column_name=column_name, active=1) hit_column_info = &#123;&#125; hit_column_info['cluster_name'] = cluster_name hit_column_info['table_schema'] = table_schema hit_column_info['table_name'] = table_name hit_column_info['column_name'] = column_name hit_column_info['rule_type'] = 0 hit_column_info['is_hit'] = False # 命中规则 if column_info: hit_column_info['rule_type'] = column_info[0].rule_type hit_column_info['is_hit'] = True return hit_column_info # 获取表中所有命中脱敏规则的字段信息 def hit_table(self, DataMaskingColumnsOb, cluster_name, table_schema, table_name): columns_info = DataMaskingColumnsOb.filter(cluster_name=cluster_name, table_schema=table_schema, table_name=table_name, active=1) # 命中规则 hit_columns_info = [] for column in columns_info: hit_column_info = &#123;&#125; hit_column_info['cluster_name'] = cluster_name hit_column_info['table_schema'] = table_schema hit_column_info['table_name'] = table_name hit_column_info['is_hit'] = True hit_column_info['column_name'] = column.column_name hit_column_info['rule_type'] = column.rule_type hit_columns_info.append(hit_column_info) return hit_columns_info # 利用正则表达式脱敏数据 def regex(self, DataMaskingRulesOb, rule_type, str): rules_info = DataMaskingRulesOb.get(rule_type=rule_type) if rules_info: rule_regex = rules_info.rule_regex hide_group = rules_info.hide_group # 正则匹配必须分组，隐藏的组会使用****代替 try: p = re.compile(rule_regex) m = p.search(str) masking_str = '' for i in range(m.lastindex): if i == hide_group-1: group = '****' else: group = m.group(i+1) masking_str = masking_str + group return masking_str except Exception: return str else: return str]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>data masking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django+gunicorn+nginx部署时，nginx反向代理非80端口时重定向报错]]></title>
    <url>%2F2018%2F04%2F16%2Fdjango-nginx-gunicorn%2F</url>
    <content type="text"><![CDATA[最近使用django+gunicorn+nginx部署archer项目时，反向代理使用的是9123端口，当访问http://127.0.0.1:9123 时会重定向到http://127.0.0.1/login/， 导致无法访问，需要手动添加端口，同时admin管理后台的跳转也是类似情况，异常麻烦，最后记录下解决方案。 settings文件增加 12# 解决nginx部署跳转404USE_X_FORWARDED_HOST = True nginx配置修改 123456789101112131415161718192021222324server&#123; listen 9123; #监听的端口 server_name archer; location / &#123; proxy_pass http://127.0.0.1:8888; proxy_set_header Host $host:9123; #这里增加监听的端口即可 proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; &#125; location /static &#123; alias /opt/archer/static; &#125; error_page 404 /404.html; location = /40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125;&#125; reload配置，重新访问，重定向时就会自动增加端口信息了]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django全局异常捕获保存和输出，logger配置]]></title>
    <url>%2F2018%2F04%2F16%2Fdjango-traceback%2F</url>
    <content type="text"><![CDATA[Middleware 123456789# -*- coding: UTF-8 -*-import logginglogger = logging.getLogger('default')class ExceptionLoggingMiddleware(object): def process_exception(self, request, exception): import traceback logger.error(traceback.format_exc()) settings 123MIDDLEWARE_CLASSES = ('sql.exception_logging_middleware.ExceptionLoggingMiddleware',) logger 12345678910111213141516171819202122232425262728293031323334353637383940414243DEFAULT_LOGS = '/tmp/default.log'stamdard_format = '[%(asctime)s][%(threadName)s:%(thread)d]' + \ '[task_id:%(name)s][%(filename)s:%(lineno)d] ' + \ '[%(levelname)s]- %(message)s'LOGGING = &#123; 'version': 1, 'disable_existing_loggers': False, 'formatters': &#123; 'standard': &#123; # 详细 'format': stamdard_format &#125;, &#125;, 'handlers': &#123; 'default': &#123; 'level': 'DEBUG', 'class': 'logging.handlers.RotatingFileHandler', 'filename': DEFAULT_LOGS, 'maxBytes': 1024 * 1024 * 100, # 5 MB 'backupCount': 5, 'formatter': 'standard', &#125;, 'console': &#123; 'level': 'DEBUG', 'class': 'logging.StreamHandler', &#125; &#125;, 'loggers': &#123; 'default': &#123; # default日志，存放于log中 'handlers': ['default'], 'level': 'DEBUG', &#125;, 'django.db': &#123; # 打印SQL语句到console，方便开发 'handlers': ['console'], 'level': 'DEBUG', 'propagate': True, &#125;, 'django.request': &#123; # 打印错误信息到console，方便开发 'handlers': ['console'], 'level': 'DEBUG', 'propagate': True, &#125;, &#125;&#125; log文件 12345678[2018-04-16 10:33:02,639][MainThread:139636491347776][task_id:default][exception_logging_middleware.py:10] [ERROR]- Traceback (most recent call last): File &quot;/opt/venv4archer/lib/python3.4/site-packages/django/core/handlers/base.py&quot;, line 132, in get_response response = wrapped_callback(request, *callback_args, **callback_kwargs) File &quot;/opt/venv4archer/lib/python3.4/site-packages/django/views/decorators/csrf.py&quot;, line 58, in wrapped_view return view_func(*args, **kwargs) File &quot;/opt/archer/sql/views_ajax.py&quot;, line 340, in getOscPercent return HttpResponse(json.dumps(pctResult), content_type=&apos;application/json&apos;)UnboundLocalError: local variable &apos;pctResult&apos; referenced before assignment]]></content>
      <categories>
        <category>django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[django获取from表单multiple-select的值]]></title>
    <url>%2F2018%2F04%2F16%2Fdjango-multiple-selecter%2F</url>
    <content type="text"><![CDATA[html代码 1234567891011&lt;!--增加通知人--&gt;&lt;div class="form-group"&gt; &lt;select id="notify_users" name="notify_users" class="selectpicker show-tick form-control bs-select-hidden" data-name="通知人" data-live-search="true" multiple="multiple" data-placeholder="请选择通知人:"&gt; &lt;option value="is-empty" disabled="" selected="selected"&gt;请选择通知人:&lt;/option&gt; &#123;% for user_info in active_user %&#125; &lt;option value="&#123;&#123; user_info.username &#125;&#125;"&gt;&#123;&#123; user_info.username &#125;&#125;&lt;/option&gt; &#123;% endfor %&#125; &lt;/select&gt;&lt;/div&gt; 后端代码 12def func(request): notify_users = request.POST.getlist('notify_users') 取值结果 1notify_users&lt;class &apos;list&apos;&gt;: [&apos;user1&apos;, &apos;user2&apos;, &apos;user3&apos;, &apos;user4&apos;, &apos;user5&apos;]]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bootstrap-table动态绑定字段]]></title>
    <url>%2F2017%2F11%2F30%2Fbootstrap-table-dynamic-binding%2F</url>
    <content type="text"><![CDATA[一个web端提交查询sql并返回表格的例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;script&gt;//将数据通过ajax提交给后端进行检查function sqlquery() &#123;var columns = [];var sqlContent = $("#sql_content");var dbName = $("#db_name");$.ajax(&#123; type: "post", url: "/do_sqlquery/", dataType: "json", data: &#123; sql_content: sqlContent.val(), db_name: dbName.val() &#125;, complete: function () &#123; $('input[type=button]').removeClass('disabled'); $('input[type=button]').addClass('btn'); $('input[type=button]').prop('disabled', false); &#125;, success: function (data) &#123; if (data.status == 0) &#123; var result = data.data; if (result['Error'])&#123; alertStyle = "alert-danger"; $('#query_result').bootstrapTable('destroy').bootstrapTable(&#123; columns: [&#123; field: 'error', title: 'Error' &#125;], data:[&#123; error: result['Error'] &#125;] &#125;) &#125; else if (result['column_list'])&#123; //获取要动态生成的列 $.each(result['column_list'], function (i,column) &#123; columns.push(&#123; "field": i ,"title": column&#125;); &#125;); //初始化table $('#query_result').bootstrapTable('destroy').bootstrapTable(&#123; data: result['rows'], columns: columns, showColumns: true, striped: true, pagination: true, pageSize:10 &#125;) &#125; //填充内容后展现出来 $("#sqlquery-result").show(); &#125; else &#123; alert("status: " + data.status + "\nmsg: " + data.msg); &#125; &#125;, error: function (XMLHttpRequest, textStatus, errorThrown) &#123; alert(errorThrown); &#125;&#125;);&#125;&lt;/script&gt;]]></content>
      <categories>
        <category>bootstrap</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[django-debug设置为False后静态文件获取404]]></title>
    <url>%2F2017%2F11%2F30%2Fdjango-debug%2F</url>
    <content type="text"><![CDATA[当设置setting.py文件当中的DEBUG=FALSE后，Django会默认使用Web Server的静态文件处理,会出现访问静态文件404的情况。 可以通过设置–insecure参数解决1python manage.py runserver --insecure 不过还是建议用nginx等来做静态资源访问，减小服务端压力]]></content>
      <categories>
        <category>django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[django时间格式化显示]]></title>
    <url>%2F2017%2F11%2F28%2Fdjango-date-format%2F</url>
    <content type="text"><![CDATA[修改settings.py,添加一下内容：USE_L10N = FalseDATETIME_FORMAT = ‘Y-m-d H:i:s’DATE_FORMAT = ‘Y-m-d’ 可全局生效，无需在每个html里面使用 1&#123;&#123; workflow.create_time|date:&quot;Y-m-d H:i:s&quot; &#125;&#125; 语法来格式化]]></content>
      <categories>
        <category>django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[利用python脚本和telnet调试dubbo接口]]></title>
    <url>%2F2017%2F10%2F20%2Fpython-dubbo-telnet%2F</url>
    <content type="text"><![CDATA[最近在测试项目中接触到dubbo框架，由于业务逻辑复杂，前台一个业务流程在后端会依赖多个服务提供数据，而各方开发进度也不完全一致。在业务测试脚本编写完成后，希望能够在项目整体提测前，先验证部分已提供的dubbo接口的可用性。另外一方面，也能够在测试过程中更快的定位到具体的服务提供者，指派任务，减少反复沟通定位问题所耗费的时间。 先说环境：OS: macOS High Sierrapython: 2.7 步骤: 准备python环境，pip安装dubbo_telnet:pip install dubbo_telnet 编写如下调试脚本 1234567891011121314151617181920212223242526272829#-*- coding: utf-8 -*-import dubbo_telnetimport jsonHost = '192.168.0.1' # Doubble服务器IPPort = 9036 # Doubble服务端口# 初始化dubbo对象conn = dubbo_telnet.connect(Host, Port)# 设置telnet连接超时时间conn.set_connect_timeout(10)# 设置dubbo服务返回响应的编码conn.set_encoding('gbk')# 显示服务列表print conn.do("ls")# 显示指定服务的方法列表print conn.do("ls XXXService")# 方法调用interface = 'XXXService'method = 'userinfo'param = user_idresult = conn.invoke(interface, method, param)print json.dumps(result, sort_keys=True, indent=4, separators=(',', ': '), skipkeys=True, ensure_ascii=False) 这时候运行脚本，抛错 123 File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.py", line 384, in raw_decode raise ValueError("No JSON object could be decoded")ValueError: No JSON object could be decoded 检查site-packages/dubbo_telnet/init.py文件 1234567data = ''while data.find(self.__finish) == -1: data = tn.read_very_eager()data = data.split("\n")data = json.loads(data[0], encoding=self.__encoding)tn.close() # tn.write('exit\n')return data 可以知道是在对返回结果处理时出了异常，无法转换为python对象，在这里加上一个错误处理 12345try: data = json.loads(data[0], encoding=self.__encoding)except Exception as ValueError: data = data#data = json.loads(data[0], encoding=self.__encoding) 就可以正常返回结果，可以进行简单的dubbo请求和查看响应了。其实整个过程就是利用telnet命令来连接和操作dubbo服务，也可以直接在命令行中进行。 附上dubbo的telnet命令： 12345678910111213Please input "help [command]" show detail. status [-l] - Show status. pwd - Print working default service. trace [service] [method] [times] - Trace the service. exit - Exit the telnet. help [command] - Show help. invoke [service.]method(args) - Invoke the service method. count [service] [method] [times] - Count the service. clear [lines] - Clear screen. ls [-l] [service] - List services and methods. log level - Change log level or show log ps [-l] [port] - Print server ports and connections. cd [service] - Change default service.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django-ajax-post请求报错]]></title>
    <url>%2F2017%2F09%2F27%2Fdjango-ajax-post%2F</url>
    <content type="text"><![CDATA[原ajax请求写法： 12345678910111213141516171819202122$.ajax(&#123; type: "post", url: "/tasks/addnewtask", dataType: "json", data: &#123; conf_name: conf_name, conf_date: conf_date, do_time: do_time &#125;, complete:function () &#123; &#125;, success:function (data) &#123; if (data.status == 0)&#123; location.reload() &#125; else &#123; alert(data.msg) &#125; &#125;, error:function (XMLHttpRequest, textStatus, errorThrown) &#123; alert(errorThrown); &#125;&#125;); 请求提示： 123456RuntimeError at /tasks/addnewtaskYou called this URL via POST, but the URL doesn&apos;t end in a slash and you have APPEND_SLASH set. Django can&apos;t redirect to the slash URLwhile maintaining POST data. Change your form to point to 127.0.0.1:8000/tasks/addnewtask/ (note the trailing slash), or set APPEND_SLASH=False in your Django settings. 修改： 在url末尾加上’/‘，修改为url: &quot;/tasks/addnewtask/&quot;,，即可恢复正常 关于APPEND_SLASH配置项： 设置项是否开启URL访问地址后面不为/跳转至带有/的路径APPEND_SLASH=True]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在macOS10.12上面搭建robotframework的RIDE环境]]></title>
    <url>%2F2017%2F09%2F16%2Frobotframework-install%2F</url>
    <content type="text"><![CDATA[安装环境准备： macOS10.13Python 2.7.10pip 安装文件： 下载地址：http://t.cn/RqSjT8j 安装步骤： 安装robotframework：pip install robotframework 解压下载后的文件wxPython-2.8.12.1.zip 拷贝pth文件文件到指定目录 使用pip show robotframework查看目录路径 1sudo cp wxredirect.pth /Library/Python/2.7/site-packages/ 拷贝wxPython目录到指定目录 如果没有/usr/local/lib/目录需要先创建 1sudo cp -r ~/Downloads/wxPython-unicode-2.8.12.1/ /usr/local/lib/wxPython-unicode-2.8.12.1/ 安装ride：pip install robotframework-ride 运行： 在终端输入：ride.py 如果提示python should be executed in 32-bit mode with wxPython on OSX. 则需要执行如下命令 1defaults write com.apple.versioner.python Prefer-32-Bit -bool yes 再次运行应该就可以正常打开ride界面，如果仍不行，可以再次尝试执行下面的安装方式，这是由于macOS系统 SIP (System Integrity Protection)的限制导致 12pip install --user robotframeworkpip install --user robotframework-ride 参考 &emsp;&emsp;安装文件和步骤来源于道长的文章：http://mp.weixin.qq.com/s/zq5OxoDnF1KbCNrvO45hag]]></content>
      <categories>
        <category>robotframework</category>
      </categories>
      <tags>
        <tag>robotframework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次MySQL表分区操作]]></title>
    <url>%2F2017%2F09%2F16%2Fpartitioned-tables%2F</url>
    <content type="text"><![CDATA[最近一次日常迭代中，业务线需要对一张大表进行联合查询，查询性能可想而知，测试过程中服务接口直接响应超时，导致服务不可用，最后临时对该表进行分区操作，暂时缓解性能问题。由于是第一次操作表分区，姑且记录一下整个操作过程。 测试表结构 12345678CREATE TABLE `tb_partition_test` ( `user_id` bigint(20) NOT NULL , `city_id` bigint(20) NOT NULL DEFAULT '0', `record_type` smallint(6) NOT NULL DEFAULT '0', `record` smallint(6) NOT NULL DEFAULT '0' , `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', PRIMARY KEY (`user_id`,`record_type`), ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='分区测试'; 表应用场景为：存储用户的成绩明细数据，成绩计算有多种不同的维度，每个用户的每个成绩维度只有一个分值。所以整个表按照record_type分组后，每个成绩对应的数据量是一致的，都为用户数量。最后对record_type进行hash分区。 为了不影响生产环境的业务使用，而恰好该表又不会有业务更新操作，故新建一个分区表，将原有表数据导入，采取分块导入的方式，避免产生大量的临时文件。 创建分区表 12345678910CREATE TABLE `tb_partition_test_part` ( `user_id` bigint(20) NOT NULL , `city_id` bigint(20) NOT NULL DEFAULT '0', `record_type` smallint(6) NOT NULL DEFAULT '0', `record` smallint(6) NOT NULL DEFAULT '0' , `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', PRIMARY KEY (`user_id`,`record_type`), ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='分区测试'PARTITION BY HASH(record_type)PARTITIONS 100; 数据导入 12345for i in &#123;1..100&#125;;do &quot;INSERT IGNORE INTO tb_partition_test_part SELECT * from tb_partition_test WHERE score_type=$i;&quot;sleep 5 done 导入完成后修改表名，会有短暂时间的锁表 123456set AUTOCOMMIT = 0;BEGIN ;RENAME TABLE tb_partition_test to tb_partition_test_20170916;RENAME TABLE tb_partition_test_part to tb_partition_test;COMMIT ;set AUTOCOMMIT = 1; 查询性能比较： 通过explain partitions查看执行计划，对于指定record_type的查询，只会在指定的分区中查找，数据按照record_type均匀分区了，前后执行时间比较 123456+----------+| count(0) |+----------+| 361795 |+----------+1 row in set (42.89 sec) 123456+----------+| count(0) |+----------+| 361795 |+----------+1 row in set (3.04 sec) 显然3.04 sec的性能仍是不可接受的，作为相对低频的业务，这只是临时的处理方案，而且恰好因为目前的需求在每次查询都会附带record_type条件，所以可以很好的利用分区提升性能，但如果出现不按照record_type条件的查询，仍然会出现性能瓶颈，需要后端人员以及来进行优化。 参考 &emsp;&emsp;1. https://dev.mysql.com/doc/refman/5.6/en/partitioning-hash.html&emsp;&emsp;2. http://www.cnblogs.com/chenmh/p/5623474.html&emsp;&emsp;3. http://blog.csdn.net/zzy7075/article/details/70054818&emsp;&emsp;4.http://blog.csdn.net/yongchao940/article/details/55266603]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>partition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL自动化运维工具Inception]]></title>
    <url>%2F2017%2F09%2F10%2Finception%2F</url>
    <content type="text"><![CDATA[Inception简介参考Inception开源GitHub地址：https://github.com/hhyo/inception.git 文档地址：https://inception-document.readthedocs.io/zh_CN/latest/ Dockerhttps://hub.docker.com/r/hhyo/inception 利用 Dockerfile 定制Inception镜像 Docker基本使用参考https://yeasy.gitbooks.io/docker_practice/content/ 使用centos作为基础镜像Dockerfile： 1234567891011121314151617181920212223FROM docker.io/centos#inceptionRUN yum -y install wget git gcc gcc-c++ make cmake openssl-devel ncurses-devel m4\ &amp;&amp; cd /opt \ &amp;&amp; git clone https://github.com/hhyo/inception.git \ &amp;&amp; rpm -i /opt/inception/dockersrc/bison-2.7-4.el7.x86_64.rpm \ &amp;&amp; mv /opt/inception/dockersrc/inc.cnf /etc \ &amp;&amp; cd inception \ &amp;&amp; ./inception_build.sh debug \ &amp;&amp; yum -y install https://repo.percona.com/yum/percona-release-latest.noarch.rpm \ &amp;&amp; yum -y install percona-toolkit \#修改中文支持 &amp;&amp; rm -rf /etc/localtime &amp;&amp; ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \ &amp;&amp; yum -y install kde-l10n-Chinese &amp;&amp; yum -y reinstall glibc-common \ &amp;&amp; localedef -c -f UTF-8 -i zh_CN zh_CN.utf8 ENV LC_ALL zh_CN.utf8 #设置环境变量#portEXPOSE 6669#start serviceENTRYPOINT nohup /opt/inception/debug/mysql/bin/Inception --defaults-file=/etc/inc.cnf &amp;&amp; bash 配置文件inc.cnf 1234567891011121314151617181920212223[inception]general_log=1general_log_file=inception.logport=6669socket=/tmp/inc.socketcharacter-set-client-handshake=0character-set-server=utf8inception_language_code=zh-CNinception_remote_system_password=rootinception_remote_system_user=rootinception_remote_backup_port=3306inception_remote_backup_host=127.0.0.1inception_support_charset=utf8,utf8mb4inception_enable_nullable=0inception_check_primary_key=1inception_check_column_comment=1inception_check_table_comment=1inception_osc_on=OFFinception_osc_bin_dir=/usr/bininception_osc_min_table_size=1inception_osc_chunk_time=0.1inception_enable_blob_type=1inception_check_column_default_value=1 构建 1docker build -t inception . 启动 1docker run --name inception -p 6669:6669 -d inception 通过MySQL客户端镜进行连接 1$ mysql -h127.0.0.1 -P6669 查看变量 1mysql&gt; inception get variables; Inception使用基本的测试脚本，验证Inception工作是否正常 12345678910111213141516171819202122#!/usr/bin/python#-\*-coding: utf-8-\*-import MySQLdbsql="/*--user=;--password=;--host=;--check=1;--port=;*/\inception_magic_start;\use mysql;\CREATE TABLE adaptive_office(id int);\inception_magic_commit;"try: conn=MySQLdb.connect(host='127.0.0.1',user='',passwd='',db='',port=6669) cur=conn.cursor() ret=cur.execute(sql) result=cur.fetchall() num_fields = len(cur.description) field_names = [i[0] for i in cur.description] print field_names for row in result: print row[0], "|",row[1],"|",row[2],"|",row[3],"|",row[4],"|",row[5],"|",row[6],"|",row[7],"|",row[8],"|",row[9],"|",row[10] cur.close() conn.close()except MySQLdb.Error,e: print "Mysql Error %d: %s" % (e.args[0], e.args[1]) web平台archeryarchery是一个基于inception的自动化SQL操作平台，支持工单、审核、认证、邮件、OSC等功能。项目地址：https://github.com/hhyo/archery 实际使用中遇到的问题日期不支持格式为全0为了前后端处理方便，业务上面会要求使用全0作为日期的默认值，但是在Inception审核时会报错 12345CREATE TABLE `test_date` ( `id` bigint(20) NOT NULL DEFAULT 0 COMMENT 'id', `create_time timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' COMMENT '创建时间', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='Inception时间格式审核测试' Invalid default value for column ‘create_time’ 参考资料：http://blog.csdn.net/achuo/article/details/54618990解决方案，修改sql/sql_parse.cc文件，来源：https://github.com/wowkingah/inception/ 12345678910 str_to_time(system_charset_info, res-&gt;ptr(), res-&gt;length(), &amp;ltime, 0, &amp;status); else str_to_datetime(system_charset_info, res-&gt;ptr(), res-&gt;length(), &amp;ltime, - MODE_NO_ZERO_DATE|MODE_NO_ZERO_IN_DATE, &amp;status);+// MODE_NO_ZERO_DATE|MODE_NO_ZERO_IN_DATE, &amp;status);+ //允许日期为全0+ NULL, &amp;status); //在上面没有检查出来的情况下，还需要对范围溢出做检查 if (status.warnings == 0) &#123; TEXT/BLOB类型的列不支持NOT NULL配置inception_check_column_default_value=1&amp;inception_enable_nullable=0时，创建TEXT/BLOB类型的列并且指定为NOT NULL，审核报错提示cant be not null 审核语句 12345CREATE TABLE `test_text` ( `id` bigint(20) NOT NULL DEFAULT 0 COMMENT 'id', `ramark text NOT NULL COMMENT '备注', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='Inception审核测试' 执行结果 TEXT/BLOB Column ‘ramark’ in table ‘test_text’ can’t been not null. 解决方案 修改sql/sql_parse.cc文件将field-&gt;flags &amp; NOT_NULL_FLAG取反 12345if (!(field-&gt;flags &amp; NOT_NULL_FLAG) &amp;&amp; mysql_field_is_blob(field-&gt;sql_type))&#123; my_error(ER_TEXT_NOT_NULLABLE_ERROR, MYF(0), field-&gt;field_name, table_name); mysql_errmsg_append(thd);&#125; 新增case ER_TEXT_NIT_NUALLABLE_ERROR 12case ER_TEXT_NOT_NULLABLE_ERROR:case ER_NOT_ALLOWED_NULLABLE: 备份语句长度超过限制解决方案，修改sql/sql_parse.cc文件，text–&gt;longtext 1create_sql-&gt;append(&quot;sql_statement text,&quot;); 变更字段类型时的默认值问题原字段 1`media_no` bigint(20) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;字段验证&apos; 变更语句 12ALTER TABLE `inception_text`MODIFY COLUMN `media_no` varchar(20) NOT NULL DEFAULT '' COMMENT '字段验证'; inception审核结果 Invalid default value for column ‘media_no’. 参考 &emsp;&emsp;1. http://www.ywnds.com/?p=9423]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>Inception</tag>
      </tags>
  </entry>
</search>
